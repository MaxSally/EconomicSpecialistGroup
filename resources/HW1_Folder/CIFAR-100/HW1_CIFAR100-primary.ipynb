{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll start with our library imports...\n",
    "from __future__ import print_function\n",
    "import pandas\n",
    "import seaborn as sn\n",
    "import numpy as np                 # to use numpy arrays\n",
    "import tensorflow as tf            # to specify and run computation graphs\n",
    "import tensorflow_datasets as tfds # to load training data\n",
    "import matplotlib.pyplot as plt    # to visualize data and draw plots\n",
    "from tqdm import tqdm              # to track progress of loops\n",
    "from keras.datasets import cifar100\n",
    "from keras import backend as k \n",
    "import keras\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import add, ZeroPadding2D, Add, GlobalAveragePooling2D, Dense, Activation, Flatten, Conv2D, MaxPooling2D, MaxPool2D, BatchNormalization, ReLU, Dropout, GlobalAvgPool2D, Input\n",
    "from functools import partial\n",
    "from keras import backend\n",
    "from tensorflow.python.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(images, labels), (temp, temp1) = cifar100.load_data()\n",
    "train_images, validation_images = images[:40000], images[40000:]\n",
    "train_labels, validation_labels = labels[:40000], labels[40000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols, img_width = 32, 32, 3\n",
    "  \n",
    "# if k.image_data_format() == 'channels_first': \n",
    "#    train_images = train_images.reshape(train_images.shape[0], 1, img_rows, img_cols, img_width) \n",
    "#    validation_images = validation_images.reshape(validation_images.shape[0], 1, img_rows, img_cols, img_width) \n",
    "#    inpx = (img_rows, img_cols, img_width)\n",
    "  \n",
    "# else: \n",
    "#    train_images = train_images.reshape(train_images.shape[0], img_rows, img_cols, img_width) \n",
    "#    validation_images = validation_images.reshape(validation_images.shape[0], img_rows, img_cols, img_width) \n",
    "#    inpx = (img_rows, img_cols, img_width) \n",
    "  \n",
    "train_images = train_images.astype('float32') \n",
    "validation_images = validation_images.astype('float32') \n",
    "temp = temp.astype('float32')\n",
    "train_images /= 255\n",
    "validation_images /= 255\n",
    "temp /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 100)\n"
     ]
    }
   ],
   "source": [
    "train_labels = keras.utils.to_categorical(train_labels)\n",
    "print(train_labels.shape)\n",
    "validation_labels_save = validation_labels\n",
    "validation_labels = keras.utils.to_categorical(validation_labels)\n",
    "temp1 = keras.utils.to_categorical(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_rows, img_cols, img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPANSION_FACTOR = 4\n",
    "DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, strides=1, padding=\"SAME\", use_bias=False)\n",
    "class Bottleneck(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, filter_num, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert filter_num % EXPANSION_FACTOR == 0\n",
    "        \n",
    "        self.conv1 = Conv2D(filter_num // EXPANSION_FACTOR, kernel_size=1, padding='same')\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.conv2 = Conv2D(filter_num // EXPANSION_FACTOR, kernel_size=3, strides=stride, padding='same')\n",
    "        self.bn2 = BatchNormalization()\n",
    "        self.conv3 = Conv2D(filter_num, kernel_size=1, padding='same')\n",
    "        self.bn3 = BatchNormalization()\n",
    "        self.dropout = Dropout(rate=0.25)\n",
    "        if self.stride != 1:\n",
    "            self.down_conv = tf.keras.layers.Conv2D(filters=filter_num,\n",
    "                                                    kernel_size=(1, 1),\n",
    "                                                    strides=stride,\n",
    "                                                    padding=\"same\")\n",
    "            self.down_bn = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def __call__(self, x, is_training = True):\n",
    "        identity = x\n",
    "        if self.stride != 1:\n",
    "            identity = self.down_conv(identity)\n",
    "            identity = self.down_bn(identity, training=is_training)\n",
    "\n",
    "        x = self.bn1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.bn2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = self.bn3(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        return x + identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 16, 16, 64)        9408      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu (TensorFlow [(None, 8, 8, 64)]        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 16)          1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 16)          64        \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_1 (TensorFl [(None, 8, 8, 16)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 16)          2320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 16)          64        \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_2 (TensorFl [(None, 8, 8, 16)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 64)          1088      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_AddV2 (TensorFlo [(None, 8, 8, 64)]        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_3 (TensorFl [(None, 8, 8, 64)]        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 16)          1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 16)          64        \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_4 (TensorFl [(None, 8, 8, 16)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 16)          2320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 16)          64        \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_5 (TensorFl [(None, 8, 8, 16)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 64)          1088      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_AddV2_1 (TensorF [(None, 8, 8, 64)]        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_6 (TensorFl [(None, 8, 8, 64)]        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 16)          1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 8, 8, 16)          64        \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_7 (TensorFl [(None, 8, 8, 16)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 16)          2320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 8, 8, 16)          64        \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_8 (TensorFl [(None, 8, 8, 16)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 64)          1088      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_AddV2_2 (TensorF [(None, 8, 8, 64)]        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_9 (TensorFl [(None, 8, 8, 64)]        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 8, 8, 32)          2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_10 (TensorF [(None, 8, 8, 32)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 4, 4, 32)          9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_11 (TensorF [(None, 4, 4, 32)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 4, 4, 128)         8320      \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 4, 4, 128)         4224      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_AddV2_3 (TensorF [(None, 4, 4, 128)]       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_12 (TensorF [(None, 4, 4, 128)]       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 4, 4, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_13 (TensorF [(None, 4, 4, 32)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 4, 4, 32)          9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_14 (TensorF [(None, 4, 4, 32)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 4, 4, 128)         4224      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_AddV2_4 (TensorF [(None, 4, 4, 128)]       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_15 (TensorF [(None, 4, 4, 128)]       0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 4, 4, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_16 (TensorF [(None, 4, 4, 32)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 4, 4, 32)          9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_17 (TensorF [(None, 4, 4, 32)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 4, 4, 128)         4224      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_AddV2_5 (TensorF [(None, 4, 4, 128)]       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_18 (TensorF [(None, 4, 4, 128)]       0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 4, 4, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_19 (TensorF [(None, 4, 4, 32)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 4, 4, 32)          9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_20 (TensorF [(None, 4, 4, 32)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 4, 4, 128)         4224      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_AddV2_6 (TensorF [(None, 4, 4, 128)]       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_21 (TensorF [(None, 4, 4, 128)]       0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 4, 4, 64)          8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_22 (TensorF [(None, 4, 4, 64)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_23 (TensorF [(None, 2, 2, 64)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 2, 2, 256)         33024     \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 2, 2, 256)         16640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 2, 2, 256)         1024      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_AddV2_7 (TensorF [(None, 2, 2, 256)]       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 2, 2, 256)         1024      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_24 (TensorF [(None, 2, 2, 256)]       0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 2, 2, 64)          16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_25 (TensorF [(None, 2, 2, 64)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_26 (TensorF [(None, 2, 2, 64)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 2, 2, 256)         16640     \n",
      "_________________________________________________________________\n",
      "tf_op_layer_AddV2_8 (TensorF [(None, 2, 2, 256)]       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 2, 2, 256)         1024      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_27 (TensorF [(None, 2, 2, 256)]       0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 2, 2, 64)          16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_28 (TensorF [(None, 2, 2, 64)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_29 (TensorF [(None, 2, 2, 64)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 2, 2, 256)         16640     \n",
      "_________________________________________________________________\n",
      "tf_op_layer_AddV2_9 (TensorF [(None, 2, 2, 256)]       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 2, 2, 256)         1024      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_30 (TensorF [(None, 2, 2, 256)]       0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 2, 2, 64)          16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_31 (TensorF [(None, 2, 2, 64)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_32 (TensorF [(None, 2, 2, 64)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 2, 2, 256)         16640     \n",
      "_________________________________________________________________\n",
      "tf_op_layer_AddV2_10 (Tensor [(None, 2, 2, 256)]       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 2, 2, 256)         1024      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_33 (TensorF [(None, 2, 2, 256)]       0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 2, 2, 64)          16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_34 (TensorF [(None, 2, 2, 64)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_35 (TensorF [(None, 2, 2, 64)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 2, 2, 256)         16640     \n",
      "_________________________________________________________________\n",
      "tf_op_layer_AddV2_11 (Tensor [(None, 2, 2, 256)]       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 2, 2, 256)         1024      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_36 (TensorF [(None, 2, 2, 256)]       0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 2, 2, 64)          16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_37 (TensorF [(None, 2, 2, 64)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_38 (TensorF [(None, 2, 2, 64)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 2, 2, 256)         16640     \n",
      "_________________________________________________________________\n",
      "tf_op_layer_AddV2_12 (Tensor [(None, 2, 2, 256)]       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 2, 2, 256)         1024      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_39 (TensorF [(None, 2, 2, 256)]       0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 2, 2, 128)         32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 2, 2, 128)         512       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_40 (TensorF [(None, 2, 2, 128)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 1, 1, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_41 (TensorF [(None, 1, 1, 128)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 1, 1, 512)         131584    \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 1, 1, 512)         66048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 1, 1, 512)         2048      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_AddV2_13 (Tensor [(None, 1, 1, 512)]       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 1, 1, 512)         2048      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_42 (TensorF [(None, 1, 1, 512)]       0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 1, 1, 128)         65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_43 (TensorF [(None, 1, 1, 128)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 1, 1, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_44 (TensorF [(None, 1, 1, 128)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 1, 1, 512)         66048     \n",
      "_________________________________________________________________\n",
      "tf_op_layer_AddV2_14 (Tensor [(None, 1, 1, 512)]       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 1, 1, 512)         2048      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_45 (TensorF [(None, 1, 1, 512)]       0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 1, 1, 128)         65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_46 (TensorF [(None, 1, 1, 128)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 1, 1, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_47 (TensorF [(None, 1, 1, 128)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 1, 1, 512)         66048     \n",
      "_________________________________________________________________\n",
      "tf_op_layer_AddV2_15 (Tensor [(None, 1, 1, 512)]       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               51300     \n",
      "=================================================================\n",
      "Total params: 1,557,060\n",
      "Trainable params: 1,544,708\n",
      "Non-trainable params: 12,352\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(DefaultConv2D(64, kernel_size=7, strides=2,\n",
    "input_shape=input_shape))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(\"relu\"))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\"))\n",
    "prev_filters = 64\n",
    "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model.add(Bottleneck(filters, stride=strides))\n",
    "    prev_filters = filters\n",
    "model.add(keras.layers.GlobalAvgPool2D())\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(100, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1250/1250 [==============================] - 78s 63ms/step - loss: 5.0273 - accuracy: 0.0239 - val_loss: 4.2436 - val_accuracy: 0.0586\n",
      "Epoch 2/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 4.3793 - accuracy: 0.0483 - val_loss: 3.9851 - val_accuracy: 0.0895\n",
      "Epoch 3/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 4.0861 - accuracy: 0.0763 - val_loss: 3.7608 - val_accuracy: 0.1247\n",
      "Epoch 4/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 3.8951 - accuracy: 0.1016 - val_loss: 3.6717 - val_accuracy: 0.1339\n",
      "Epoch 5/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 3.7561 - accuracy: 0.1228 - val_loss: 3.5353 - val_accuracy: 0.1632\n",
      "Epoch 6/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 3.6485 - accuracy: 0.1408 - val_loss: 3.4180 - val_accuracy: 0.1814\n",
      "Epoch 7/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 3.5460 - accuracy: 0.1595 - val_loss: 3.3444 - val_accuracy: 0.1949\n",
      "Epoch 8/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 3.4639 - accuracy: 0.1728 - val_loss: 3.2618 - val_accuracy: 0.2099\n",
      "Epoch 9/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 3.3861 - accuracy: 0.1912 - val_loss: 3.2094 - val_accuracy: 0.2197\n",
      "Epoch 10/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 3.3188 - accuracy: 0.1990 - val_loss: 3.1257 - val_accuracy: 0.2352\n",
      "Epoch 11/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 3.2461 - accuracy: 0.2105 - val_loss: 3.0804 - val_accuracy: 0.2432\n",
      "Epoch 12/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 3.1791 - accuracy: 0.2239 - val_loss: 3.0253 - val_accuracy: 0.2552\n",
      "Epoch 13/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 3.1222 - accuracy: 0.2343 - val_loss: 2.9666 - val_accuracy: 0.2668\n",
      "Epoch 14/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 3.0638 - accuracy: 0.2456 - val_loss: 2.9109 - val_accuracy: 0.2724\n",
      "Epoch 15/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 3.0195 - accuracy: 0.2558 - val_loss: 2.8723 - val_accuracy: 0.2880\n",
      "Epoch 16/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.9700 - accuracy: 0.2656 - val_loss: 2.8350 - val_accuracy: 0.2922\n",
      "Epoch 17/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.9231 - accuracy: 0.2715 - val_loss: 2.8001 - val_accuracy: 0.3057\n",
      "Epoch 18/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.8903 - accuracy: 0.2820 - val_loss: 2.7583 - val_accuracy: 0.3133\n",
      "Epoch 19/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.8489 - accuracy: 0.2889 - val_loss: 2.7367 - val_accuracy: 0.3122\n",
      "Epoch 20/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.8219 - accuracy: 0.2931 - val_loss: 2.7122 - val_accuracy: 0.3150\n",
      "Epoch 21/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.7844 - accuracy: 0.2982 - val_loss: 2.6721 - val_accuracy: 0.3259\n",
      "Epoch 22/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.7465 - accuracy: 0.3057 - val_loss: 2.6397 - val_accuracy: 0.3345\n",
      "Epoch 23/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.7167 - accuracy: 0.3156 - val_loss: 2.6312 - val_accuracy: 0.3393\n",
      "Epoch 24/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.6958 - accuracy: 0.3169 - val_loss: 2.6131 - val_accuracy: 0.3373\n",
      "Epoch 25/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.6641 - accuracy: 0.3239 - val_loss: 2.5861 - val_accuracy: 0.3445\n",
      "Epoch 26/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.6447 - accuracy: 0.3274 - val_loss: 2.5725 - val_accuracy: 0.3479\n",
      "Epoch 27/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.6207 - accuracy: 0.3346 - val_loss: 2.5545 - val_accuracy: 0.3533\n",
      "Epoch 28/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.5959 - accuracy: 0.3365 - val_loss: 2.5420 - val_accuracy: 0.3574\n",
      "Epoch 29/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.5695 - accuracy: 0.3431 - val_loss: 2.5337 - val_accuracy: 0.3618\n",
      "Epoch 30/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.5499 - accuracy: 0.3459 - val_loss: 2.5105 - val_accuracy: 0.3666\n",
      "Epoch 31/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.5352 - accuracy: 0.3530 - val_loss: 2.4929 - val_accuracy: 0.3627\n",
      "Epoch 32/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.5093 - accuracy: 0.3563 - val_loss: 2.4690 - val_accuracy: 0.3742\n",
      "Epoch 33/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.4915 - accuracy: 0.3569 - val_loss: 2.4691 - val_accuracy: 0.3692\n",
      "Epoch 34/500\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 2.4716 - accuracy: 0.3640 - val_loss: 2.4794 - val_accuracy: 0.3703\n",
      "Epoch 35/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.4609 - accuracy: 0.3696 - val_loss: 2.4406 - val_accuracy: 0.3802\n",
      "Epoch 36/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.4346 - accuracy: 0.3690 - val_loss: 2.4550 - val_accuracy: 0.3734\n",
      "Epoch 37/500\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 2.4250 - accuracy: 0.3722 - val_loss: 2.4356 - val_accuracy: 0.3787\n",
      "Epoch 38/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.4039 - accuracy: 0.3773 - val_loss: 2.4117 - val_accuracy: 0.3857\n",
      "Epoch 39/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.3814 - accuracy: 0.3818 - val_loss: 2.4120 - val_accuracy: 0.3805\n",
      "Epoch 40/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.3682 - accuracy: 0.3842 - val_loss: 2.3917 - val_accuracy: 0.3909\n",
      "Epoch 41/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.3579 - accuracy: 0.3878 - val_loss: 2.4054 - val_accuracy: 0.3863\n",
      "Epoch 42/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.3347 - accuracy: 0.3929 - val_loss: 2.4084 - val_accuracy: 0.3877\n",
      "Epoch 43/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.3191 - accuracy: 0.3957 - val_loss: 2.3812 - val_accuracy: 0.3923\n",
      "Epoch 44/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.3073 - accuracy: 0.3959 - val_loss: 2.3945 - val_accuracy: 0.3886\n",
      "Epoch 45/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.3074 - accuracy: 0.3964 - val_loss: 2.3754 - val_accuracy: 0.3947\n",
      "Epoch 46/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.2781 - accuracy: 0.4026 - val_loss: 2.3722 - val_accuracy: 0.3943\n",
      "Epoch 47/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.2774 - accuracy: 0.4029 - val_loss: 2.3607 - val_accuracy: 0.3946\n",
      "Epoch 48/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.2611 - accuracy: 0.4069 - val_loss: 2.3570 - val_accuracy: 0.3983\n",
      "Epoch 49/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.2535 - accuracy: 0.4120 - val_loss: 2.3311 - val_accuracy: 0.4046\n",
      "Epoch 50/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.2339 - accuracy: 0.4143 - val_loss: 2.3629 - val_accuracy: 0.3999\n",
      "Epoch 51/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.2229 - accuracy: 0.4171 - val_loss: 2.3257 - val_accuracy: 0.4079\n",
      "Epoch 52/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.2118 - accuracy: 0.4211 - val_loss: 2.3326 - val_accuracy: 0.4032\n",
      "Epoch 53/500\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 2.1997 - accuracy: 0.4211 - val_loss: 2.3172 - val_accuracy: 0.4101\n",
      "Epoch 54/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.1873 - accuracy: 0.4238 - val_loss: 2.3261 - val_accuracy: 0.4086\n",
      "Epoch 55/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.1793 - accuracy: 0.4248 - val_loss: 2.3148 - val_accuracy: 0.4108\n",
      "Epoch 56/500\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 2.1688 - accuracy: 0.4297 - val_loss: 2.3213 - val_accuracy: 0.4094\n",
      "Epoch 57/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.1548 - accuracy: 0.4322 - val_loss: 2.3157 - val_accuracy: 0.4112\n",
      "Epoch 58/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.1446 - accuracy: 0.4350 - val_loss: 2.3118 - val_accuracy: 0.4106\n",
      "Epoch 59/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.1301 - accuracy: 0.4354 - val_loss: 2.3004 - val_accuracy: 0.4126\n",
      "Epoch 60/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.1254 - accuracy: 0.4394 - val_loss: 2.2973 - val_accuracy: 0.4164\n",
      "Epoch 61/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.1195 - accuracy: 0.4390 - val_loss: 2.2953 - val_accuracy: 0.4107\n",
      "Epoch 62/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.1182 - accuracy: 0.4395 - val_loss: 2.2948 - val_accuracy: 0.4210\n",
      "Epoch 63/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.1059 - accuracy: 0.4419 - val_loss: 2.3755 - val_accuracy: 0.3982\n",
      "Epoch 64/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.0890 - accuracy: 0.4473 - val_loss: 2.2949 - val_accuracy: 0.4175\n",
      "Epoch 65/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.0724 - accuracy: 0.4493 - val_loss: 2.3214 - val_accuracy: 0.4148\n",
      "Epoch 66/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.0688 - accuracy: 0.4484 - val_loss: 2.2760 - val_accuracy: 0.4211\n",
      "Epoch 67/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.0629 - accuracy: 0.4516 - val_loss: 2.2773 - val_accuracy: 0.4242\n",
      "Epoch 68/500\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 2.0592 - accuracy: 0.4515 - val_loss: 2.2778 - val_accuracy: 0.4239\n",
      "Epoch 69/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.0558 - accuracy: 0.4538 - val_loss: 2.2991 - val_accuracy: 0.4142\n",
      "Epoch 70/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.0441 - accuracy: 0.4567 - val_loss: 2.2590 - val_accuracy: 0.4224\n",
      "Epoch 71/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.0327 - accuracy: 0.4554 - val_loss: 2.2684 - val_accuracy: 0.4228\n",
      "Epoch 72/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.0176 - accuracy: 0.4610 - val_loss: 2.2637 - val_accuracy: 0.4216\n",
      "Epoch 73/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.0195 - accuracy: 0.4608 - val_loss: 2.2550 - val_accuracy: 0.4270\n",
      "Epoch 74/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 2.0177 - accuracy: 0.4629 - val_loss: 2.2927 - val_accuracy: 0.4148\n",
      "Epoch 75/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.9971 - accuracy: 0.4653 - val_loss: 2.2609 - val_accuracy: 0.4228\n",
      "Epoch 76/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.9890 - accuracy: 0.4678 - val_loss: 2.2569 - val_accuracy: 0.4257\n",
      "Epoch 77/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.9768 - accuracy: 0.4720 - val_loss: 2.2593 - val_accuracy: 0.4227\n",
      "Epoch 78/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.9805 - accuracy: 0.4724 - val_loss: 2.2546 - val_accuracy: 0.4251\n",
      "Epoch 79/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.9765 - accuracy: 0.4702 - val_loss: 2.2703 - val_accuracy: 0.4206\n",
      "Epoch 80/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.9726 - accuracy: 0.4717 - val_loss: 2.2741 - val_accuracy: 0.4188\n",
      "Epoch 81/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.9597 - accuracy: 0.4755 - val_loss: 2.2670 - val_accuracy: 0.4245\n",
      "Epoch 82/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.9618 - accuracy: 0.4737 - val_loss: 2.2596 - val_accuracy: 0.4235\n",
      "Epoch 83/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.9463 - accuracy: 0.4759 - val_loss: 2.2685 - val_accuracy: 0.4253\n",
      "Epoch 84/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.9401 - accuracy: 0.4786 - val_loss: 2.2433 - val_accuracy: 0.4286\n",
      "Epoch 85/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.9291 - accuracy: 0.4827 - val_loss: 2.2775 - val_accuracy: 0.4213\n",
      "Epoch 86/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.9282 - accuracy: 0.4830 - val_loss: 2.2508 - val_accuracy: 0.4289\n",
      "Epoch 87/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.9156 - accuracy: 0.4823 - val_loss: 2.2643 - val_accuracy: 0.4257\n",
      "Epoch 88/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.9193 - accuracy: 0.4832 - val_loss: 2.2726 - val_accuracy: 0.4225\n",
      "Epoch 89/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.9016 - accuracy: 0.4879 - val_loss: 2.2427 - val_accuracy: 0.4288\n",
      "Epoch 90/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.9054 - accuracy: 0.4852 - val_loss: 2.2509 - val_accuracy: 0.4256\n",
      "Epoch 91/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.8983 - accuracy: 0.4872 - val_loss: 2.2350 - val_accuracy: 0.4339\n",
      "Epoch 92/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.8835 - accuracy: 0.4915 - val_loss: 2.2443 - val_accuracy: 0.4326\n",
      "Epoch 93/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.8774 - accuracy: 0.4905 - val_loss: 2.2496 - val_accuracy: 0.4308\n",
      "Epoch 94/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.8856 - accuracy: 0.4893 - val_loss: 2.2541 - val_accuracy: 0.4244\n",
      "Epoch 95/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.8698 - accuracy: 0.4929 - val_loss: 2.2596 - val_accuracy: 0.4248\n",
      "Epoch 96/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.8642 - accuracy: 0.4968 - val_loss: 2.2488 - val_accuracy: 0.4348\n",
      "Epoch 97/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.8631 - accuracy: 0.4960 - val_loss: 2.2462 - val_accuracy: 0.4312\n",
      "Epoch 98/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.8513 - accuracy: 0.4953 - val_loss: 2.2410 - val_accuracy: 0.4323\n",
      "Epoch 99/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.8410 - accuracy: 0.5023 - val_loss: 2.2663 - val_accuracy: 0.4307\n",
      "Epoch 100/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.8432 - accuracy: 0.5009 - val_loss: 2.2437 - val_accuracy: 0.4343\n",
      "Epoch 101/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.8375 - accuracy: 0.4990 - val_loss: 2.2623 - val_accuracy: 0.4297\n",
      "Epoch 102/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.8399 - accuracy: 0.5022 - val_loss: 2.2241 - val_accuracy: 0.4373\n",
      "Epoch 103/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.8303 - accuracy: 0.5039 - val_loss: 2.2305 - val_accuracy: 0.4396\n",
      "Epoch 104/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.8245 - accuracy: 0.5043 - val_loss: 2.2344 - val_accuracy: 0.4326\n",
      "Epoch 105/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.8184 - accuracy: 0.5065 - val_loss: 2.2342 - val_accuracy: 0.4364\n",
      "Epoch 106/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.8186 - accuracy: 0.5088 - val_loss: 2.2526 - val_accuracy: 0.4309\n",
      "Epoch 107/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.8177 - accuracy: 0.5049 - val_loss: 2.2425 - val_accuracy: 0.4323\n",
      "Epoch 108/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.8095 - accuracy: 0.5064 - val_loss: 2.2309 - val_accuracy: 0.4350\n",
      "Epoch 109/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.8108 - accuracy: 0.5076 - val_loss: 2.2219 - val_accuracy: 0.4382\n",
      "Epoch 110/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.7943 - accuracy: 0.5096 - val_loss: 2.2280 - val_accuracy: 0.4375\n",
      "Epoch 111/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.7958 - accuracy: 0.5103 - val_loss: 2.2234 - val_accuracy: 0.4417\n",
      "Epoch 112/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.7812 - accuracy: 0.5157 - val_loss: 2.2484 - val_accuracy: 0.4373\n",
      "Epoch 113/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.7906 - accuracy: 0.5099 - val_loss: 2.2599 - val_accuracy: 0.4340\n",
      "Epoch 114/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.7881 - accuracy: 0.5109 - val_loss: 2.2249 - val_accuracy: 0.4379\n",
      "Epoch 115/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.7750 - accuracy: 0.5150 - val_loss: 2.2321 - val_accuracy: 0.4356\n",
      "Epoch 116/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.7706 - accuracy: 0.5127 - val_loss: 2.2305 - val_accuracy: 0.4398\n",
      "Epoch 117/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.7683 - accuracy: 0.5166 - val_loss: 2.2430 - val_accuracy: 0.4393\n",
      "Epoch 118/500\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 1.7519 - accuracy: 0.5219 - val_loss: 2.2349 - val_accuracy: 0.4374\n",
      "Epoch 121/500\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 1.7484 - accuracy: 0.5201 - val_loss: 2.2367 - val_accuracy: 0.4388\n",
      "Epoch 122/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.7511 - accuracy: 0.5231 - val_loss: 2.2374 - val_accuracy: 0.4379\n",
      "Epoch 123/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.7420 - accuracy: 0.5231 - val_loss: 2.2410 - val_accuracy: 0.4380\n",
      "Epoch 124/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.7332 - accuracy: 0.5261 - val_loss: 2.2292 - val_accuracy: 0.4398\n",
      "Epoch 125/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.7386 - accuracy: 0.5232 - val_loss: 2.2328 - val_accuracy: 0.4378\n",
      "Epoch 126/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.7225 - accuracy: 0.5249 - val_loss: 2.2304 - val_accuracy: 0.4405\n",
      "Epoch 127/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.7189 - accuracy: 0.5283 - val_loss: 2.2340 - val_accuracy: 0.4404\n",
      "Epoch 128/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.7239 - accuracy: 0.5280 - val_loss: 2.2282 - val_accuracy: 0.4420\n",
      "Epoch 129/500\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 1.7096 - accuracy: 0.5312 - val_loss: 2.2350 - val_accuracy: 0.4426\n",
      "Epoch 130/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.7123 - accuracy: 0.5299 - val_loss: 2.2303 - val_accuracy: 0.4435\n",
      "Epoch 131/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.7119 - accuracy: 0.5274 - val_loss: 2.2340 - val_accuracy: 0.4404\n",
      "Epoch 132/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6980 - accuracy: 0.5346 - val_loss: 2.2194 - val_accuracy: 0.4483\n",
      "Epoch 133/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.7089 - accuracy: 0.5308 - val_loss: 2.2293 - val_accuracy: 0.4428\n",
      "Epoch 134/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6996 - accuracy: 0.5311 - val_loss: 2.2381 - val_accuracy: 0.4404\n",
      "Epoch 135/500\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 1.6928 - accuracy: 0.5336 - val_loss: 2.2390 - val_accuracy: 0.4417\n",
      "Epoch 136/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6914 - accuracy: 0.5351 - val_loss: 2.2304 - val_accuracy: 0.4440\n",
      "Epoch 137/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6916 - accuracy: 0.5356 - val_loss: 2.2362 - val_accuracy: 0.4416\n",
      "Epoch 138/500\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 1.6940 - accuracy: 0.5328 - val_loss: 2.2360 - val_accuracy: 0.4414\n",
      "Epoch 139/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6923 - accuracy: 0.5357 - val_loss: 2.2572 - val_accuracy: 0.4405\n",
      "Epoch 140/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6791 - accuracy: 0.5379 - val_loss: 2.2367 - val_accuracy: 0.4467\n",
      "Epoch 141/500\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 1.6826 - accuracy: 0.5372 - val_loss: 2.2277 - val_accuracy: 0.4438\n",
      "Epoch 142/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6814 - accuracy: 0.5384 - val_loss: 2.2358 - val_accuracy: 0.4428\n",
      "Epoch 143/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6739 - accuracy: 0.5422 - val_loss: 2.2470 - val_accuracy: 0.4347\n",
      "Epoch 144/500\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 1.6686 - accuracy: 0.5415 - val_loss: 2.2380 - val_accuracy: 0.4395\n",
      "Epoch 145/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6605 - accuracy: 0.5395 - val_loss: 2.2404 - val_accuracy: 0.4411\n",
      "Epoch 146/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6563 - accuracy: 0.5424 - val_loss: 2.2201 - val_accuracy: 0.4442\n",
      "Epoch 147/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6520 - accuracy: 0.5428 - val_loss: 2.2266 - val_accuracy: 0.4463\n",
      "Epoch 148/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6586 - accuracy: 0.5424 - val_loss: 2.2192 - val_accuracy: 0.4476\n",
      "Epoch 149/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6543 - accuracy: 0.5436 - val_loss: 2.2370 - val_accuracy: 0.4416\n",
      "Epoch 150/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6545 - accuracy: 0.5459 - val_loss: 2.2396 - val_accuracy: 0.4507\n",
      "Epoch 151/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6391 - accuracy: 0.5462 - val_loss: 2.2321 - val_accuracy: 0.4428\n",
      "Epoch 152/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6431 - accuracy: 0.5494 - val_loss: 2.2348 - val_accuracy: 0.4451\n",
      "Epoch 153/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6344 - accuracy: 0.5481 - val_loss: 2.2400 - val_accuracy: 0.4416\n",
      "Epoch 154/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6385 - accuracy: 0.5487 - val_loss: 2.2265 - val_accuracy: 0.4453\n",
      "Epoch 155/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6397 - accuracy: 0.5455 - val_loss: 2.2420 - val_accuracy: 0.4403\n",
      "Epoch 156/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6333 - accuracy: 0.5477 - val_loss: 2.2375 - val_accuracy: 0.4461\n",
      "Epoch 157/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6241 - accuracy: 0.5522 - val_loss: 2.2308 - val_accuracy: 0.4461\n",
      "Epoch 158/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6277 - accuracy: 0.5490 - val_loss: 2.2300 - val_accuracy: 0.4438\n",
      "Epoch 159/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6164 - accuracy: 0.5504 - val_loss: 2.2421 - val_accuracy: 0.4473\n",
      "Epoch 160/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6297 - accuracy: 0.5480 - val_loss: 2.2365 - val_accuracy: 0.4445\n",
      "Epoch 161/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6100 - accuracy: 0.5522 - val_loss: 2.2310 - val_accuracy: 0.4474\n",
      "Epoch 162/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6176 - accuracy: 0.5548 - val_loss: 2.2275 - val_accuracy: 0.4459\n",
      "Epoch 163/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6136 - accuracy: 0.5513 - val_loss: 2.2362 - val_accuracy: 0.4449\n",
      "Epoch 164/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6047 - accuracy: 0.5563 - val_loss: 2.2292 - val_accuracy: 0.4453\n",
      "Epoch 165/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.6108 - accuracy: 0.5527 - val_loss: 2.2385 - val_accuracy: 0.4454\n",
      "Epoch 166/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.5967 - accuracy: 0.5583 - val_loss: 2.2284 - val_accuracy: 0.4456\n",
      "Epoch 167/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.5974 - accuracy: 0.5565 - val_loss: 2.2413 - val_accuracy: 0.4419\n",
      "Epoch 168/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.5930 - accuracy: 0.5554 - val_loss: 2.2605 - val_accuracy: 0.4432\n",
      "Epoch 169/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.5911 - accuracy: 0.5583 - val_loss: 2.2412 - val_accuracy: 0.4425\n",
      "Epoch 170/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.5971 - accuracy: 0.5581 - val_loss: 2.2339 - val_accuracy: 0.4444\n",
      "Epoch 171/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.5813 - accuracy: 0.5615 - val_loss: 2.2353 - val_accuracy: 0.4467\n",
      "Epoch 172/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.5851 - accuracy: 0.5628 - val_loss: 2.2400 - val_accuracy: 0.4442\n",
      "Epoch 173/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.5822 - accuracy: 0.5615 - val_loss: 2.2368 - val_accuracy: 0.4440\n",
      "Epoch 174/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.5794 - accuracy: 0.5594 - val_loss: 2.2524 - val_accuracy: 0.4430\n",
      "Epoch 175/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.5754 - accuracy: 0.5638 - val_loss: 2.2525 - val_accuracy: 0.4444\n",
      "Epoch 176/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.5830 - accuracy: 0.5569 - val_loss: 2.2372 - val_accuracy: 0.4450\n",
      "Epoch 177/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.5735 - accuracy: 0.5623 - val_loss: 2.2370 - val_accuracy: 0.4471\n",
      "Epoch 178/500\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 1.5738 - accuracy: 0.5616 - val_loss: 2.2410 - val_accuracy: 0.4460\n"
     ]
    }
   ],
   "source": [
    "# using Sequential groups all the layers to run at once\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', patience = 30)\n",
    "history = model.fit(train_images, train_labels, batch_size = 32, epochs=500, validation_data=(validation_images, validation_labels), callbacks = [es], shuffle=True, use_multiprocessing=(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 2.2410 - accuracy: 0.4460\n"
     ]
    }
   ],
   "source": [
    "validation_evaluation = model.evaluate(validation_images, validation_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 2.2023 - accuracy: 0.4556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2022705078125, 0.45559999346733093]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(temp, temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(validation_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[76 26  2 ... 88  6 73]\n"
     ]
    }
   ],
   "source": [
    "y_prediction_bin = np.array([])\n",
    "\n",
    "for example in y_predict:\n",
    "    maxN = example[0]\n",
    "    ind = 0\n",
    "    for i in range(1, 100):\n",
    "        if example[i] > maxN:\n",
    "            ind = i\n",
    "            maxN = example[i]\n",
    "    y_prediction_bin = np.append(y_prediction_bin, ind)\n",
    "\n",
    "y_prediction_bin = y_prediction_bin.astype(int)\n",
    "print(y_prediction_bin) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[65  0  1 ...  0  0  0]\n",
      " [ 0 68  0 ...  0  0  0]\n",
      " [ 0  1 42 ...  1  4  0]\n",
      " ...\n",
      " [ 0  0  2 ... 55  0  0]\n",
      " [ 0  0 14 ...  1 25  0]\n",
      " [ 1  0  0 ...  0  1 26]], shape=(100, 100), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = tf.math.confusion_matrix(validation_labels_save, y_prediction_bin)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"HW1_model2_confusion_matrix.txt\", confusion_matrix.numpy(), fmt='%03.d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 95% Confidence interval for error hypothesis based on the normal distribution estimator:  (0.5442573114430266, 0.5637426690066499)\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "validation_data_error = 1 - validation_evaluation[1]\n",
    "\n",
    "lower_bound_interval = validation_data_error - 1.96 * sqrt( (validation_data_error * (1 - validation_data_error)) / len(validation_labels))\n",
    "upper_bound_interval = validation_data_error + 1.96 * sqrt( (validation_data_error * (1 - validation_data_error)) / len(validation_labels))\n",
    "\n",
    "print(\"The 95% Confidence interval for error hypothesis based on the normal distribution estimator: \", (lower_bound_interval, upper_bound_interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_WEIGHT_DECAY = 0.2\n",
    "BATCH_NORM_DECAY = 0.99\n",
    "BATCH_NORM_EPSILON = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, kernel_size, filters):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if backend.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "\n",
    "    x =  Conv2D(filters1, (1, 1), use_bias=False,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(input_tensor)\n",
    "\n",
    "    x =  BatchNormalization(axis=bn_axis,\n",
    "                                  momentum=BATCH_NORM_DECAY,\n",
    "                                  epsilon=BATCH_NORM_EPSILON)(x)\n",
    "    x =  Activation('relu')(x)\n",
    "\n",
    "    x =  Conv2D(filters2, kernel_size,\n",
    "                      padding='same', use_bias=False,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(x)\n",
    "\n",
    "    x =  BatchNormalization(axis=bn_axis,\n",
    "                                  momentum=BATCH_NORM_DECAY,\n",
    "                                  epsilon=BATCH_NORM_EPSILON)(x)\n",
    "\n",
    "    x =  Activation('relu')(x)\n",
    "\n",
    "    x =  Conv2D(filters3, (1, 1), use_bias=False,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(x)\n",
    "\n",
    "    x =  BatchNormalization(axis=bn_axis,\n",
    "                                  momentum=BATCH_NORM_DECAY,\n",
    "                                  epsilon=BATCH_NORM_EPSILON)(x)\n",
    "\n",
    "    x =  add([x, input_tensor])\n",
    "    x =  Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, kernel_size, filters, strides=(2, 2)):\n",
    "    \"\"\"A block that has a conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    Note that from stage 3,\n",
    "    the second conv layer at main path is with strides=(2, 2)\n",
    "    And the shortcut should have strides=(2, 2) as well\n",
    "    \"\"\"\n",
    "\n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    if backend.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "\n",
    "    x =  Conv2D(filters1, (1, 1), use_bias=False,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(input_tensor)\n",
    "    x =  BatchNormalization(axis=bn_axis,\n",
    "                                  momentum=BATCH_NORM_DECAY,\n",
    "                                  epsilon=BATCH_NORM_EPSILON)(x)\n",
    "    x =  Activation('relu')(x)\n",
    "\n",
    "\n",
    "    x =  Conv2D(filters2, kernel_size, strides=strides, padding='same',\n",
    "                      use_bias=False, kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(x)\n",
    "    x =  BatchNormalization(axis=bn_axis,\n",
    "                                  momentum=BATCH_NORM_DECAY,\n",
    "                                  epsilon=BATCH_NORM_EPSILON)(x)\n",
    "    x =  Activation('relu')(x)\n",
    "\n",
    "    x =  Conv2D(filters3, (1, 1), use_bias=False,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(x)\n",
    "    x =  BatchNormalization(axis=bn_axis,\n",
    "                                  momentum=BATCH_NORM_DECAY,\n",
    "                                  epsilon=BATCH_NORM_EPSILON)(x)\n",
    "\n",
    "    shortcut =  Conv2D(filters3, (1, 1), strides=strides, use_bias=False,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(input_tensor)\n",
    "    shortcut =  BatchNormalization(axis=bn_axis,\n",
    "                                         momentum=BATCH_NORM_DECAY,\n",
    "                                         epsilon=BATCH_NORM_EPSILON)(shortcut)\n",
    "\n",
    "    x =  add([x, shortcut])\n",
    "    x =  Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet50(num_classes, input_shape):\n",
    "    img_input = Input(shape=input_shape)\n",
    "    \n",
    "    if backend.image_data_format() == 'channels_first':\n",
    "        x = Lambda(lambda x: backend.permute_dimensions(x, (0, 3, 1, 2)),\n",
    "                          name='transpose')(img_input)\n",
    "        bn_axis = 1\n",
    "    else:  # channels_last\n",
    "        x = img_input\n",
    "        bn_axis = 3\n",
    "\n",
    "    # Conv1 (7x7,64,stride=2)\n",
    "    x = ZeroPadding2D(padding=(3, 3))(x)\n",
    "\n",
    "    x = Conv2D(64, (7, 7),\n",
    "                      strides=(2, 2),\n",
    "                      padding='valid', use_bias=False,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(x)\n",
    "    x = BatchNormalization(axis=bn_axis,\n",
    "                                  momentum=BATCH_NORM_DECAY,\n",
    "                                  epsilon=BATCH_NORM_EPSILON)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "\n",
    "    # 3x3 max pool,stride=2\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    # Conv2_x\n",
    "\n",
    "    # 1×1, 64\n",
    "    # 3×3, 64\n",
    "    # 1×1, 256\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256])\n",
    "    x = identity_block(x, 3, [64, 64, 256])\n",
    "\n",
    "    # Conv3_x\n",
    "    #\n",
    "    # 1×1, 128\n",
    "    # 3×3, 128\n",
    "    # 1×1, 512\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512])\n",
    "    x = identity_block(x, 3, [128, 128, 512])\n",
    "    x = identity_block(x, 3, [128, 128, 512])\n",
    "    x = identity_block(x, 3, [128, 128, 512])\n",
    "\n",
    "    # Conv4_x\n",
    "    # 1×1, 256\n",
    "    # 3×3, 256\n",
    "    # 1×1, 1024\n",
    "    #x = conv_block(x, 3, [256, 256, 1024])\n",
    "    #x = identity_block(x, 3, [256, 256, 1024])\n",
    "    #x = identity_block(x, 3, [256, 256, 1024])\n",
    "    #x = identity_block(x, 3, [256, 256, 1024])\n",
    "    #x = identity_block(x, 3, [256, 256, 1024])\n",
    "    #x = identity_block(x, 3, [256, 256, 1024])\n",
    "\n",
    "    # 1×1, 512\n",
    "    # 3×3, 512\n",
    "    # 1×1, 2048\n",
    "    #x = conv_block(x, 3, [512, 512, 2048])\n",
    "    #x = identity_block(x, 3, [512, 512, 2048])\n",
    "    #x = identity_block(x, 3, [512, 512, 2048])\n",
    "    \n",
    "    # average pool, 100-d fc, softmax\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(\n",
    "        num_classes, activation='softmax',\n",
    "        kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n",
    "        bias_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(x)\n",
    "\n",
    "    # Create model.\n",
    "    return Model(img_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(100, input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Sequential groups all the layers to run at once\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', patience = 30)\n",
    "history = model.fit(train_images, train_labels, batch_size = 32, epochs=400, validation_data=(validation_images, validation_labels), callbacks = [es], shuffle=True, use_multiprocessing=(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(model, 'homework1/cifar100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-env)",
   "language": "python",
   "name": "tensorflow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
