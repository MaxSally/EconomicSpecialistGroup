{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-5b2d9f765668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 12288)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               6291968   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 6,423,553\n",
      "Trainable params: 6,423,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 128)       51328     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 3)         9603      \n",
      "=================================================================\n",
      "Total params: 16,859,139\n",
      "Trainable params: 16,850,691\n",
      "Non-trainable params: 8,448\n",
      "_________________________________________________________________\n",
      "0\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'model_json' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-e25ff08bff0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-e25ff08bff0a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, save_interval)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0mmodel_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                 \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0;31m# serialize weights to HDF5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'model_json' referenced before assignment"
     ]
    }
   ],
   "source": [
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 64 \n",
    "        self.img_cols = 64\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        # Build and compile the generator\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(4096,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator) takes\n",
    "        # noise as input => generates images => determines validity \n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "        \n",
    "    def build_generator(self):\n",
    "        noise_shape = (4096,)\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        divisor = 4\n",
    "        model.add(Dense(16 * (self.img_rows // divisor) * (self.img_cols // divisor), input_shape=noise_shape))\n",
    "        model.add(LeakyReLU(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Reshape((self.img_rows // divisor, self.img_cols // divisor, 16)))\n",
    "\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, (5,5), padding='same'))\n",
    "        model.add(LeakyReLU(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(self.channels, (5,5), padding='same', activation='tanh'))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=noise_shape)\n",
    "        img = model(noise)\n",
    "        return Model(noise, img)\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "        img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=img_shape)\n",
    "        validity = model(img)\n",
    "        return Model(img, validity)\n",
    "    def get_image(self, image_path, width, height, mode):  \n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize([width, height])\n",
    "        #print(\"img\",image)\n",
    "        return np.array(image.convert(mode))\n",
    "\n",
    "    def get_batch(self, image_files, width, height, mode):\n",
    "        #print(image_files)\n",
    "        data_batch = np.array([self.get_image(sample_file, width, height, mode) for sample_file in image_files])\n",
    "        \n",
    "        return data_batch    \n",
    "    \n",
    "    def add_noise(self,image):\n",
    "        ch = 3\n",
    "        row,col = 64,64\n",
    "        #print(row,col,ch)\n",
    "        mean = 0\n",
    "        var = 0.1\n",
    "        sigma = var**0.5\n",
    "        gauss = np.random.normal(mean,sigma,(row,col,ch))\n",
    "        gauss = gauss.reshape(row,col,ch)\n",
    "        #print(gauss.shape)\n",
    "        noisy = image + gauss\n",
    "        #print(gauss.shape)\n",
    "        #plt.imshow(noisy)\n",
    "        #plt.show()\n",
    "        #print(noisy.shape)\n",
    "        image = cv2.resize(noisy,(64, 64))    \n",
    "        return image\n",
    "    \n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "        data_dir = \"img_align_celeba/img_align_celeba\"\n",
    "        filepaths=os.listdir(data_dir)\n",
    "        \n",
    "        #print(X_train)\n",
    "        #Rescale -1 to 1\n",
    "        \n",
    "        half_batch = int(batch_size / 2)\n",
    "        #Create lists for logging the losses\n",
    "        d_loss_logs_r = []\n",
    "        d_loss_logs_f = []\n",
    "        g_loss_logs = []\n",
    "        n_iterations=math.floor(len(filepaths)/batch_size)\n",
    "        print(n_iterations)\n",
    "        for epoch in range(epochs):\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            # Select a random half batch of images\n",
    "            for ite in range(n_iterations):\n",
    "                X_train=np.array([self.add_noise(image) for image in X_train])\n",
    "                X_train = self.get_batch(glob(os.path.join(data_dir, '*.jpg'))[ite*batch_size:(ite+1)*batch_size], 64, 64, 'RGB')\n",
    "                X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "                X_train=np.array([self.add_noise(image) for image in X_train])\n",
    "                print(X_train.shape[0])\n",
    "                idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "                imgs = X_train[idx]\n",
    "                noise = np.random.normal(0, 1, (half_batch, 4096))\n",
    "                # Generate a half batch of new images\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "                # Train the discriminator\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "                noise = np.random.normal(0, 1, (batch_size, 4096))\n",
    "                #np.random.normal(0, 1, (batch_size, 4096))\n",
    "                # The generator wants the discriminator to label the generated samples\n",
    "                # as valid (ones)\n",
    "                valid_y = np.array([1] * batch_size)\n",
    "                # Train the generator\n",
    "                g_loss = self.combined.train_on_batch(noise, valid_y)\n",
    "                # Plot the progress\n",
    "                print (\"%d %d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch,ite, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "                #Append the logs with the loss values in each training step\n",
    "                d_loss_logs_r.append([epoch, d_loss[0]])\n",
    "                d_loss_logs_f.append([epoch, d_loss[1]])\n",
    "                g_loss_logs.append([epoch, g_loss])\n",
    "                \n",
    "                d_loss_logs_r_a = np.array(d_loss_logs_r)\n",
    "                d_loss_logs_f_a = np.array(d_loss_logs_f)\n",
    "                g_loss_logs_a = np.array(g_loss_logs)\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if ite % save_interval == 0:\n",
    "                    self.save_imgs(epoch,ite)\n",
    "\n",
    "                    plt.plot(d_loss_logs_r_a[:,0], d_loss_logs_r_a[:,1], label=\"Discriminator Loss - Real\")\n",
    "                    plt.plot(d_loss_logs_f_a[:,0], d_loss_logs_f_a[:,1], label=\"Discriminator Loss - Fake\")\n",
    "                    plt.plot(g_loss_logs_a[:,0], g_loss_logs_a[:,1], label=\"Generator Loss\")\n",
    "                    plt.xlabel('Epochs-iterations')\n",
    "                    plt.ylabel('Loss')\n",
    "                    plt.legend()\n",
    "                    plt.title('Variation of losses over epochs')\n",
    "                    plt.grid(True)\n",
    "                    plt.show()\n",
    "                    \n",
    "                model_json = self.generator.to_json()\n",
    "            with open(\"model\"+str(epoch)+\".json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "            # serialize weights to HDF5\n",
    "            self.generator.save_weights(\"model\"+str(epoch)+\".h5\")\n",
    "            print(\"Saved model to disk\")\n",
    "        '''\n",
    "        for i,i_path in enumerate(os.listdir('../input/sketch-to-images-resized-photos2/resized photos2zip/new_imgs')):\n",
    "            if i < 25:\n",
    "                path = os.path.join('../input/sketch-to-images-resized-photos2/resized photos2zip/new_imgs/'+i_path)\n",
    "    \n",
    "                img = cv2.imread(path,0)\n",
    "    \n",
    "                img= cv2.resize(img,(64,64))\n",
    "                img=img.reshape((1,4096))\n",
    "                gen_imgs = self.generator.predict(img)\n",
    "                gen_imgs1 = (1/2.5) * gen_imgs[0] + 0.5\n",
    "                #fig.savefig(\"%d.png\" % epoch)\n",
    "                print(gen_imgs.shape)\n",
    "                cv2.imwrite(\"gen_imgs\"+i_path,gen_imgs)\n",
    "                cv2.imwrite(\"gen_imgs1\"+i_path,gen_imgs1)\n",
    "                plt.imshow(gen_imgs[0])\n",
    "                plt.show()\n",
    "        '''\n",
    "        \n",
    "    def save_imgs(self, epoch,iteration):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, 4096))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = (1/2.5) * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,:])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(str(epoch)+\"-\"+str(iteration)+\".png\")\n",
    "        plt.close()\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs=6, batch_size=256, save_interval=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-env)",
   "language": "python",
   "name": "tensorflow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
