{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-m87i8bf0 because the default path (/home/cse479/izzatadly/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import tensorflow as tf\n",
    "#from scipy.misc import imread, imsave\n",
    "#import cv2  Cannot import\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_6 (Flatten)          (None, 12288)             0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 512)               6291968   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 6,423,553\n",
      "Trainable params: 6,423,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 12288)             50343936  \n",
      "_________________________________________________________________\n",
      "reshape_6 (Reshape)          (None, 64, 64, 3)         0         \n",
      "=================================================================\n",
      "Total params: 67,125,248\n",
      "Trainable params: 67,125,248\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/img_align_celeba/img_align_celeba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-19d27f1f6303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-19d27f1f6303>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, save_interval)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/img_align_celeba/img_align_celeba\"\u001b[0m \u001b[0;31m# Cannot seem to find my folder on crane for some reason.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mfilepaths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m#print(X_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/img_align_celeba/img_align_celeba'"
     ]
    }
   ],
   "source": [
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 64 \n",
    "        self.img_cols = 64\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "        # Build and compile the generator\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(4096,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator) takes\n",
    "        # noise as input => generates images => determines validity \n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        noise_shape = (4096,)\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        divisor = 4\n",
    "        model.add(Dense(16 * (self.img_rows // divisor) * (self.img_cols // divisor), input_shape=noise_shape))\n",
    "        model.add(LeakyReLU(0.2))\n",
    "\n",
    "        model.add(Dense(64 * 64 * 3, activation = \"sigmoid\"))\n",
    "        model.add(Reshape([64, 64, 3]))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=noise_shape)\n",
    "        img = model(noise)\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=img_shape)\n",
    "        validity = model(img)\n",
    "        return Model(img, validity)\n",
    "    \n",
    "    def get_image(self, image_path, width, height, mode):  \n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize([width, height])\n",
    "        #print(\"img\",image)\n",
    "        return np.array(image.convert(mode))\n",
    "\n",
    "    def get_batch(self, image_files, width, height, mode):\n",
    "        #print(image_files)\n",
    "        data_batch = np.array([self.get_image(sample_file, width, height, mode) for sample_file in image_files])\n",
    "        \n",
    "        return data_batch    \n",
    "    def add_noise(self,image):\n",
    "        ch = 3\n",
    "        row,col = 64,64\n",
    "        #print(row,col,ch)\n",
    "        mean = 0\n",
    "        var = 0.1\n",
    "        sigma = var**0.5\n",
    "        gauss = np.random.normal(mean,sigma,(row,col,ch))\n",
    "        gauss = gauss.reshape(row,col,ch)\n",
    "        #print(gauss.shape)\n",
    "        noisy = image + gauss\n",
    "        #print(gauss.shape)\n",
    "        #plt.imshow(noisy)\n",
    "        #plt.show()\n",
    "        #print(noisy.shape)\n",
    "        image = Image.resize(noisy,(64, 64))    \n",
    "        return image\n",
    "    \n",
    "    def plot(d_loss_logs_r_a,d_loss_logs_f_a,g_loss_logs_a):\n",
    "        #Generate the plot at the end of training\n",
    "        #Convert the log lists to numpy arrays\n",
    "        d_loss_logs_r_a = np.array(d_loss_logs_r_a)\n",
    "        d_loss_logs_f_a = np.array(d_loss_logs_f_a)\n",
    "        g_loss_logs_a = np.array(g_loss_logs_a)\n",
    "        plt.plot(d_loss_logs_r_a[:,0], d_loss_logs_r_a[:,1], label=\"Discriminator Loss - Real\")\n",
    "        plt.plot(d_loss_logs_f_a[:,0], d_loss_logs_f_a[:,1], label=\"Discriminator Loss - Fake\")\n",
    "        plt.plot(g_loss_logs_a[:,0], g_loss_logs_a[:,1], label=\"Generator Loss\")\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Variation of losses over epochs')\n",
    "        plt.grid(True)\n",
    "        plt.show()    \n",
    "        \n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "        data_dir = \"/img_align_celeba/img_align_celeba\" # Cannot seem to find my folder on crane for some reason.\n",
    "        filepaths=os.listdir(data_dir)\n",
    "        \n",
    "        #print(X_train)\n",
    "        #Rescale -1 to 1\n",
    "        \n",
    "        half_batch = int(batch_size / 2)\n",
    "        #Create lists for logging the losses\n",
    "        d_loss_logs_r = []\n",
    "        d_loss_logs_f = []\n",
    "        g_loss_logs = []\n",
    "        n_iterations=math.floor(len(filepaths)/batch_size)\n",
    "        print(n_iterations)\n",
    "        for epoch in range(epochs):\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            # Select a random half batch of images\n",
    "            for ite in range(n_iterations):\n",
    "#                 X_train=np.array([self.add_noise(image) for image in X_train])\n",
    "                X_train = self.get_batch(glob(os.path.join(data_dir, '*.jpg'))[ite*batch_size:(ite+1)*batch_size], 64, 64, 'RGB')\n",
    "                X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "                X_train=np.array([self.add_noise(image) for image in X_train])\n",
    "                print(X_train.shape[0])\n",
    "                idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "                imgs = X_train[idx]\n",
    "                noise = np.random.normal(0, 1, (half_batch, 4096))\n",
    "                # Generate a half batch of new images\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "                # Train the discriminator\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "                noise = np.random.normal(0, 1, (batch_size, 4096))\n",
    "                #np.random.normal(0, 1, (batch_size, 4096))\n",
    "                # The generator wants the discriminator to label the generated samples\n",
    "                # as valid (ones)\n",
    "                valid_y = np.array([1] * batch_size)\n",
    "                # Train the generator\n",
    "                g_loss = self.combined.train_on_batch(noise, valid_y)\n",
    "                # Plot the progress\n",
    "                print (\"%d %d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch,ite, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "                #Append the logs with the loss values in each training step\n",
    "                d_loss_logs_r.append([epoch, d_loss[0]])\n",
    "                d_loss_logs_f.append([epoch, d_loss[1]])\n",
    "                g_loss_logs.append([epoch, g_loss])\n",
    "\n",
    "                d_loss_logs_r_a = np.array(d_loss_logs_r)\n",
    "                d_loss_logs_f_a = np.array(d_loss_logs_f)\n",
    "                g_loss_logs_a = np.array(g_loss_logs)\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if ite % save_interval == 0:\n",
    "                    self.save_imgs(epoch,ite)\n",
    "\n",
    "                    plt.plot(d_loss_logs_r_a[:,0], d_loss_logs_r_a[:,1], label=\"Discriminator Loss - Real\")\n",
    "                    plt.plot(d_loss_logs_f_a[:,0], d_loss_logs_f_a[:,1], label=\"Discriminator Loss - Fake\")\n",
    "                    plt.plot(g_loss_logs_a[:,0], g_loss_logs_a[:,1], label=\"Generator Loss\")\n",
    "                    plt.xlabel('Epochs-iterations')\n",
    "                    plt.ylabel('Loss')\n",
    "                    plt.legend()\n",
    "                    plt.title('Variation of losses over epochs')\n",
    "                    plt.grid(True)\n",
    "                    plt.show()    \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            model_json = self.generator.to_json()\n",
    "            \n",
    "            # Error with this part\n",
    "            with open(\"model\"+str(epoch)+\".json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "                \n",
    "                \n",
    "            # serialize weights to HDF5\n",
    "            self.generator.save_weights(\"model\"+str(epoch)+\".h5\")\n",
    "            print(\"Saved model to disk\")\n",
    "        '''\n",
    "        for i,i_path in enumerate(os.listdir('../input/sketch-to-images-resized-photos2/resized photos2zip/new_imgs')):\n",
    "            if i < 25:\n",
    "                path = os.path.join('../input/sketch-to-images-resized-photos2/resized photos2zip/new_imgs/'+i_path)\n",
    "    \n",
    "                img = cv2.imread(path,0)\n",
    "    \n",
    "                img= cv2.resize(img,(64,64))\n",
    "                img=img.reshape((1,4096))\n",
    "                gen_imgs = self.generator.predict(img)\n",
    "                gen_imgs1 = (1/2.5) * gen_imgs[0] + 0.5\n",
    "                #fig.savefig(\"%d.png\" % epoch)\n",
    "                print(gen_imgs.shape)\n",
    "                cv2.imwrite(\"gen_imgs\"+i_path,gen_imgs)\n",
    "                cv2.imwrite(\"gen_imgs1\"+i_path,gen_imgs1)\n",
    "                plt.imshow(gen_imgs[0])\n",
    "                plt.show()\n",
    "        '''\n",
    "        \n",
    "    def save_imgs(self, epoch,iteration):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, 4096))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = (1/2.5) * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,:])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(str(epoch)+\"-\"+str(iteration)+\".png\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs=6, batch_size=10, save_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-env)",
   "language": "python",
   "name": "tensorflow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
