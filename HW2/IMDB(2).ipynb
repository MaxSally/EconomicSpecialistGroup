{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-480azb9k because the default path (/home/cse479/rbockmon2/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from __future__ import print_function\n",
    "import numpy as np                 # to use numpy arrays\n",
    "import tensorflow as tf            # to specify and run computation graphs\n",
    "import tensorflow_datasets as tfds # to load training data\n",
    "import keras\n",
    "from keras import backend\n",
    "from tensorflow.keras import Model, initializers, regularizers, constraints\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Embedding, GlobalMaxPooling1D, Dense, Flatten, Conv2D, MaxPooling2D, MaxPool2D, Dropout, GlobalAvgPool2D, Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import *\n",
    "from sklearn.model_selection import KFold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = tfds.load(\n",
    "    name=\"imdb_reviews\", \n",
    "    split=('train', 'test'),\n",
    "    as_supervised=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = train_data.as_numpy_iterator()\n",
    "train_data_2 = []\n",
    "train_label_2 = []\n",
    "for it in temp:\n",
    "    train_data_2.append(it['text'])\n",
    "    train_label_2.append(it['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = test_data.as_numpy_iterator()\n",
    "test_data_2 = []\n",
    "test_label_2 = []\n",
    "for it in temp:\n",
    "    test_data_2.append(it['text'])\n",
    "    test_label_2.append(it['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWithContext(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, W_regularizer=None, u_regularizer=None, b_regularizer=None,                \n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape = (input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name = 'name',\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape = (input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name = 'name',\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight(shape = (input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name = 'name',\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "        \n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = tf.math.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = tf.math.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= tf.cast(mask, tf.float32)\n",
    "        \n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= tf.cast(tf.math.reduce_sum(a, axis=1, keepdims=True) + backend.epsilon(), tf.float32)\n",
    "\n",
    "        a = tf.expand_dims(a, -1)\n",
    "        print(x.shape)\n",
    "        print(a.shape)\n",
    "        weighted_input = x * a\n",
    "        return tf.math.reduce_sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    return tf.tensordot(x, kernel, axes = 1)\n",
    "    #return tf.squeeze(tf.keras.layers.dot(inputs = [x, kernel], axes=1))\n",
    "    #return K.dot(x, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 60000\n",
    "maxlen = 250\n",
    "encode_dim = 70\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer()\n",
    "cnt = 0\n",
    "cnt_1 = 0\n",
    "for it in train_data_2:\n",
    "    if cnt % 1000 == 0:\n",
    "        #print(str(it))\n",
    "        cnt_1 += 1\n",
    "        #print(cnt_1)\n",
    "    cnt += 1\n",
    "    tokenizer.fit_on_texts(str(it))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "tokenized_word_list = []\n",
    "for it in train_data_2:\n",
    "    if cnt % 1000 == 0:\n",
    "        print(len(tokenized_word_list))\n",
    "    cnt += 1\n",
    "    tokenized_word_list.append(tokenizer.texts_to_sequences(str(it)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_word_list_2 = []\n",
    "for it in train_data_2:\n",
    "    temp = tokenizer.texts_to_sequences(str(it))\n",
    "    newL = []\n",
    "    for it2 in temp:\n",
    "        if it2 == []:\n",
    "            continue\n",
    "        newL.append(it2[0])\n",
    "    tokenized_word_list_2.append(newL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded = pad_sequences(tokenized_word_list_2, maxlen = maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_padded))\n",
    "print(X_train_padded.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 5)\n",
    "mc = ModelCheckpoint('model_best.h5', monitor = 'val_acc', mode = 'max', verbose = 1, save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 250, 400)\n",
      "(None, 250, 1)\n",
      "Fold : 0\n",
      "16667 16667\n",
      "Epoch 1/50\n",
      "(None, 250, 400)\n",
      "(None, 250, 1)\n",
      "(None, 250, 400)\n",
      "(None, 250, 1)\n",
      "521/521 [==============================] - ETA: 0s - loss: 0.6919 - accuracy: 0.5166(None, 250, 400)\n",
      "(None, 250, 1)\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.6919 - accuracy: 0.5166 - val_loss: 0.6818 - val_accuracy: 0.5484\n",
      "Epoch 2/50\n",
      "521/521 [==============================] - 81s 156ms/step - loss: 0.6801 - accuracy: 0.5692 - val_loss: 0.6716 - val_accuracy: 0.5851\n",
      "Epoch 3/50\n",
      "521/521 [==============================] - 80s 153ms/step - loss: 0.6756 - accuracy: 0.5796 - val_loss: 0.6790 - val_accuracy: 0.5562\n",
      "Epoch 4/50\n",
      "521/521 [==============================] - 81s 155ms/step - loss: 0.6729 - accuracy: 0.5817 - val_loss: 0.6670 - val_accuracy: 0.5968\n",
      "Epoch 5/50\n",
      "521/521 [==============================] - 81s 156ms/step - loss: 0.6718 - accuracy: 0.5836 - val_loss: 0.6791 - val_accuracy: 0.5680\n",
      "Epoch 6/50\n",
      "521/521 [==============================] - 81s 155ms/step - loss: 0.6700 - accuracy: 0.5878 - val_loss: 0.6720 - val_accuracy: 0.5787\n",
      "Epoch 7/50\n",
      "521/521 [==============================] - 81s 155ms/step - loss: 0.6657 - accuracy: 0.5927 - val_loss: 0.6545 - val_accuracy: 0.6132\n",
      "Epoch 8/50\n",
      "521/521 [==============================] - 81s 155ms/step - loss: 0.6529 - accuracy: 0.6098 - val_loss: 0.6766 - val_accuracy: 0.5795\n",
      "Epoch 9/50\n",
      "521/521 [==============================] - 81s 155ms/step - loss: 0.6456 - accuracy: 0.6092 - val_loss: 0.6298 - val_accuracy: 0.6367\n",
      "Epoch 10/50\n",
      "521/521 [==============================] - 81s 155ms/step - loss: 0.6351 - accuracy: 0.6243 - val_loss: 0.6356 - val_accuracy: 0.6149\n",
      "Epoch 11/50\n",
      "521/521 [==============================] - 81s 156ms/step - loss: 0.6244 - accuracy: 0.6355 - val_loss: 0.6142 - val_accuracy: 0.6467\n",
      "Epoch 12/50\n",
      "521/521 [==============================] - 81s 156ms/step - loss: 0.6164 - accuracy: 0.6411 - val_loss: 0.6021 - val_accuracy: 0.6509\n",
      "Epoch 13/50\n",
      "521/521 [==============================] - 81s 156ms/step - loss: 0.6099 - accuracy: 0.6447 - val_loss: 0.6073 - val_accuracy: 0.6611\n",
      "Epoch 14/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.6041 - accuracy: 0.6531 - val_loss: 0.5914 - val_accuracy: 0.6665\n",
      "Epoch 15/50\n",
      "521/521 [==============================] - 81s 155ms/step - loss: 0.6007 - accuracy: 0.6565 - val_loss: 0.6022 - val_accuracy: 0.6549\n",
      "Epoch 16/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.5912 - accuracy: 0.6677 - val_loss: 0.5802 - val_accuracy: 0.6737\n",
      "Epoch 17/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.5879 - accuracy: 0.6701 - val_loss: 0.5776 - val_accuracy: 0.6826\n",
      "Epoch 18/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.5808 - accuracy: 0.6760 - val_loss: 0.5681 - val_accuracy: 0.6889\n",
      "Epoch 19/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.5700 - accuracy: 0.6846 - val_loss: 0.5668 - val_accuracy: 0.6948\n",
      "Epoch 20/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.5629 - accuracy: 0.6934 - val_loss: 0.5542 - val_accuracy: 0.7002\n",
      "Epoch 21/50\n",
      "521/521 [==============================] - 81s 156ms/step - loss: 0.5531 - accuracy: 0.7010 - val_loss: 0.5556 - val_accuracy: 0.7090\n",
      "Epoch 22/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.5408 - accuracy: 0.7146 - val_loss: 0.5473 - val_accuracy: 0.7123\n",
      "Epoch 23/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.5297 - accuracy: 0.7201 - val_loss: 0.5424 - val_accuracy: 0.7197\n",
      "Epoch 24/50\n",
      "521/521 [==============================] - 81s 156ms/step - loss: 0.5233 - accuracy: 0.7230 - val_loss: 0.5350 - val_accuracy: 0.7177\n",
      "Epoch 25/50\n",
      "521/521 [==============================] - 81s 155ms/step - loss: 0.5118 - accuracy: 0.7337 - val_loss: 0.5581 - val_accuracy: 0.7131\n",
      "Epoch 26/50\n",
      "521/521 [==============================] - 81s 155ms/step - loss: 0.5006 - accuracy: 0.7440 - val_loss: 0.5138 - val_accuracy: 0.7325\n",
      "Epoch 27/50\n",
      "521/521 [==============================] - 80s 154ms/step - loss: 0.4930 - accuracy: 0.7487 - val_loss: 0.5245 - val_accuracy: 0.7260\n",
      "Epoch 28/50\n",
      "521/521 [==============================] - 81s 155ms/step - loss: 0.4824 - accuracy: 0.7556 - val_loss: 0.5053 - val_accuracy: 0.7411\n",
      "Epoch 29/50\n",
      "521/521 [==============================] - 80s 154ms/step - loss: 0.4721 - accuracy: 0.7655 - val_loss: 0.4985 - val_accuracy: 0.7462\n",
      "Epoch 30/50\n",
      "521/521 [==============================] - 81s 156ms/step - loss: 0.4566 - accuracy: 0.7718 - val_loss: 0.5593 - val_accuracy: 0.7303\n",
      "Epoch 31/50\n",
      "521/521 [==============================] - 80s 154ms/step - loss: 0.4496 - accuracy: 0.7764 - val_loss: 0.4964 - val_accuracy: 0.7488\n",
      "Epoch 32/50\n",
      "521/521 [==============================] - 81s 155ms/step - loss: 0.4374 - accuracy: 0.7865 - val_loss: 0.5042 - val_accuracy: 0.7485\n",
      "Epoch 33/50\n",
      "521/521 [==============================] - 81s 155ms/step - loss: 0.4250 - accuracy: 0.7913 - val_loss: 0.5026 - val_accuracy: 0.7529\n",
      "Epoch 34/50\n",
      "521/521 [==============================] - 81s 155ms/step - loss: 0.4136 - accuracy: 0.8000 - val_loss: 0.5028 - val_accuracy: 0.7577\n",
      "Epoch 35/50\n",
      "521/521 [==============================] - 81s 155ms/step - loss: 0.4001 - accuracy: 0.8073 - val_loss: 0.5086 - val_accuracy: 0.7557\n",
      "Epoch 36/50\n",
      "521/521 [==============================] - 81s 155ms/step - loss: 0.3873 - accuracy: 0.8205 - val_loss: 0.5207 - val_accuracy: 0.7584\n",
      "Epoch 00036: early stopping\n",
      "(None, 250, 400)\n",
      "(None, 250, 1)\n",
      "Fold : 1\n",
      "16667 16667\n",
      "Epoch 1/50\n",
      "(None, 250, 400)\n",
      "(None, 250, 1)\n",
      "(None, 250, 400)\n",
      "(None, 250, 1)\n",
      "521/521 [==============================] - ETA: 0s - loss: 0.6916 - accuracy: 0.5180(None, 250, 400)\n",
      "(None, 250, 1)\n",
      "521/521 [==============================] - 83s 159ms/step - loss: 0.6916 - accuracy: 0.5180 - val_loss: 0.6759 - val_accuracy: 0.5955\n",
      "Epoch 2/50\n",
      "521/521 [==============================] - 81s 156ms/step - loss: 0.6800 - accuracy: 0.5676 - val_loss: 0.6725 - val_accuracy: 0.5849\n",
      "Epoch 3/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.6757 - accuracy: 0.5792 - val_loss: 0.6739 - val_accuracy: 0.5811\n",
      "Epoch 4/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.6724 - accuracy: 0.5865 - val_loss: 0.6652 - val_accuracy: 0.6007\n",
      "Epoch 5/50\n",
      "521/521 [==============================] - 81s 156ms/step - loss: 0.6677 - accuracy: 0.5927 - val_loss: 0.6659 - val_accuracy: 0.5877\n",
      "Epoch 6/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.6592 - accuracy: 0.6033 - val_loss: 0.6458 - val_accuracy: 0.6096\n",
      "Epoch 7/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.6475 - accuracy: 0.6169 - val_loss: 0.6292 - val_accuracy: 0.6359\n",
      "Epoch 8/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.6392 - accuracy: 0.6215 - val_loss: 0.6233 - val_accuracy: 0.6371\n",
      "Epoch 9/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.6345 - accuracy: 0.6293 - val_loss: 0.6244 - val_accuracy: 0.6305\n",
      "Epoch 10/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.6254 - accuracy: 0.6347 - val_loss: 0.6058 - val_accuracy: 0.6595\n",
      "Epoch 11/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.6128 - accuracy: 0.6492 - val_loss: 0.6017 - val_accuracy: 0.6606\n",
      "Epoch 12/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.6034 - accuracy: 0.6568 - val_loss: 0.5935 - val_accuracy: 0.6725\n",
      "Epoch 13/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.5960 - accuracy: 0.6678 - val_loss: 0.5877 - val_accuracy: 0.6787\n",
      "Epoch 14/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.5910 - accuracy: 0.6705 - val_loss: 0.5807 - val_accuracy: 0.6861\n",
      "Epoch 15/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.5840 - accuracy: 0.6767 - val_loss: 0.5606 - val_accuracy: 0.7015\n",
      "Epoch 16/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.5744 - accuracy: 0.6839 - val_loss: 0.5573 - val_accuracy: 0.7018\n",
      "Epoch 17/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.5666 - accuracy: 0.6941 - val_loss: 0.5587 - val_accuracy: 0.7033\n",
      "Epoch 18/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.5590 - accuracy: 0.6993 - val_loss: 0.5424 - val_accuracy: 0.7168\n",
      "Epoch 19/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.5466 - accuracy: 0.7110 - val_loss: 0.5263 - val_accuracy: 0.7291\n",
      "Epoch 20/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.5382 - accuracy: 0.7172 - val_loss: 0.5237 - val_accuracy: 0.7293\n",
      "Epoch 21/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.5274 - accuracy: 0.7248 - val_loss: 0.5263 - val_accuracy: 0.7187\n",
      "Epoch 22/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.5157 - accuracy: 0.7355 - val_loss: 0.5010 - val_accuracy: 0.7457\n",
      "Epoch 23/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.5042 - accuracy: 0.7446 - val_loss: 0.4985 - val_accuracy: 0.7446\n",
      "Epoch 24/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.4922 - accuracy: 0.7480 - val_loss: 0.4900 - val_accuracy: 0.7547\n",
      "Epoch 25/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.4768 - accuracy: 0.7612 - val_loss: 0.4844 - val_accuracy: 0.7580\n",
      "Epoch 26/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.4727 - accuracy: 0.7641 - val_loss: 0.4930 - val_accuracy: 0.7577\n",
      "Epoch 27/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.4628 - accuracy: 0.7692 - val_loss: 0.4900 - val_accuracy: 0.7557\n",
      "Epoch 28/50\n",
      "521/521 [==============================] - 81s 156ms/step - loss: 0.4500 - accuracy: 0.7731 - val_loss: 0.4742 - val_accuracy: 0.7626\n",
      "Epoch 29/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.4415 - accuracy: 0.7822 - val_loss: 0.4818 - val_accuracy: 0.7626\n",
      "Epoch 30/50\n",
      "521/521 [==============================] - 81s 156ms/step - loss: 0.4295 - accuracy: 0.7923 - val_loss: 0.4789 - val_accuracy: 0.7626\n",
      "Epoch 31/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.4151 - accuracy: 0.8034 - val_loss: 0.4868 - val_accuracy: 0.7685\n",
      "Epoch 32/50\n",
      "521/521 [==============================] - 81s 156ms/step - loss: 0.4056 - accuracy: 0.8066 - val_loss: 0.4782 - val_accuracy: 0.7714\n",
      "Epoch 33/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.3953 - accuracy: 0.8118 - val_loss: 0.4849 - val_accuracy: 0.7701\n",
      "Epoch 00033: early stopping\n",
      "(None, 250, 400)\n",
      "(None, 250, 1)\n",
      "Fold : 2\n",
      "16666 16666\n",
      "Epoch 1/50\n",
      "(None, 250, 400)\n",
      "(None, 250, 1)\n",
      "(None, 250, 400)\n",
      "(None, 250, 1)\n",
      "521/521 [==============================] - ETA: 0s - loss: 0.6917 - accuracy: 0.5171(None, 250, 400)\n",
      "(None, 250, 1)\n",
      "521/521 [==============================] - 83s 159ms/step - loss: 0.6917 - accuracy: 0.5171 - val_loss: 0.6832 - val_accuracy: 0.5746\n",
      "Epoch 2/50\n",
      "521/521 [==============================] - 81s 156ms/step - loss: 0.6753 - accuracy: 0.5850 - val_loss: 0.6712 - val_accuracy: 0.5851\n",
      "Epoch 3/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.6720 - accuracy: 0.5861 - val_loss: 0.6867 - val_accuracy: 0.5263\n",
      "Epoch 4/50\n",
      "521/521 [==============================] - 81s 156ms/step - loss: 0.6714 - accuracy: 0.5866 - val_loss: 0.6734 - val_accuracy: 0.5821\n",
      "Epoch 5/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.6755 - accuracy: 0.5881 - val_loss: 0.6903 - val_accuracy: 0.5516\n",
      "Epoch 6/50\n",
      "521/521 [==============================] - 81s 156ms/step - loss: 0.6654 - accuracy: 0.5950 - val_loss: 0.6635 - val_accuracy: 0.6036\n",
      "Epoch 7/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.6563 - accuracy: 0.6088 - val_loss: 0.6476 - val_accuracy: 0.6088\n",
      "Epoch 8/50\n",
      "521/521 [==============================] - 81s 156ms/step - loss: 0.6447 - accuracy: 0.6207 - val_loss: 0.6336 - val_accuracy: 0.6308\n",
      "Epoch 9/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.6340 - accuracy: 0.6254 - val_loss: 0.6209 - val_accuracy: 0.6405\n",
      "Epoch 10/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.6339 - accuracy: 0.6235 - val_loss: 0.6316 - val_accuracy: 0.6156\n",
      "Epoch 11/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.6323 - accuracy: 0.6204 - val_loss: 0.6152 - val_accuracy: 0.6437\n",
      "Epoch 12/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.6207 - accuracy: 0.6410 - val_loss: 0.6159 - val_accuracy: 0.6517\n",
      "Epoch 13/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.6153 - accuracy: 0.6417 - val_loss: 0.6061 - val_accuracy: 0.6489\n",
      "Epoch 14/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.6102 - accuracy: 0.6483 - val_loss: 0.5982 - val_accuracy: 0.6597\n",
      "Epoch 15/50\n",
      "521/521 [==============================] - 83s 158ms/step - loss: 0.6031 - accuracy: 0.6562 - val_loss: 0.6004 - val_accuracy: 0.6616\n",
      "Epoch 16/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.5945 - accuracy: 0.6655 - val_loss: 0.5847 - val_accuracy: 0.6745\n",
      "Epoch 17/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.5867 - accuracy: 0.6725 - val_loss: 0.5766 - val_accuracy: 0.6813\n",
      "Epoch 18/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.5818 - accuracy: 0.6757 - val_loss: 0.5773 - val_accuracy: 0.6824\n",
      "Epoch 19/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.5728 - accuracy: 0.6886 - val_loss: 0.5690 - val_accuracy: 0.6913\n",
      "Epoch 20/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.5656 - accuracy: 0.6931 - val_loss: 0.5625 - val_accuracy: 0.6928\n",
      "Epoch 21/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.5554 - accuracy: 0.6996 - val_loss: 0.5615 - val_accuracy: 0.7106\n",
      "Epoch 22/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.5447 - accuracy: 0.7095 - val_loss: 0.5401 - val_accuracy: 0.7130\n",
      "Epoch 23/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.5321 - accuracy: 0.7198 - val_loss: 0.5325 - val_accuracy: 0.7184\n",
      "Epoch 24/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.5200 - accuracy: 0.7272 - val_loss: 0.5417 - val_accuracy: 0.7103\n",
      "Epoch 25/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.5112 - accuracy: 0.7353 - val_loss: 0.5291 - val_accuracy: 0.7297\n",
      "Epoch 26/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.4956 - accuracy: 0.7447 - val_loss: 0.5155 - val_accuracy: 0.7298\n",
      "Epoch 27/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.4901 - accuracy: 0.7539 - val_loss: 0.5185 - val_accuracy: 0.7305\n",
      "Epoch 28/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.4775 - accuracy: 0.7601 - val_loss: 0.5053 - val_accuracy: 0.7394\n",
      "Epoch 29/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.4631 - accuracy: 0.7703 - val_loss: 0.5114 - val_accuracy: 0.7475\n",
      "Epoch 30/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.4552 - accuracy: 0.7758 - val_loss: 0.4991 - val_accuracy: 0.7443\n",
      "Epoch 31/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.4475 - accuracy: 0.7786 - val_loss: 0.5042 - val_accuracy: 0.7501\n",
      "Epoch 32/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.4330 - accuracy: 0.7905 - val_loss: 0.5422 - val_accuracy: 0.7287\n",
      "Epoch 33/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.4254 - accuracy: 0.7971 - val_loss: 0.4966 - val_accuracy: 0.7570\n",
      "Epoch 34/50\n",
      "521/521 [==============================] - 82s 157ms/step - loss: 0.4148 - accuracy: 0.8006 - val_loss: 0.5429 - val_accuracy: 0.7490\n",
      "Epoch 35/50\n",
      "521/521 [==============================] - 82s 158ms/step - loss: 0.4022 - accuracy: 0.8087 - val_loss: 0.4988 - val_accuracy: 0.7568\n",
      "Epoch 36/50\n",
      "122/521 [======>.......................] - ETA: 55s - loss: 0.3764 - accuracy: 0.8233"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1e81ab278286>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_label_2_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_final2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_final2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tensorflow-env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow-env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow-env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow-env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow-env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow-env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/.conda/envs/tensorflow-env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.conda/envs/tensorflow-env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow-env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model 1\n",
    "#k-fold cross validation\n",
    "k = 3 #number of folds\n",
    "numfold = 0 #what fold we are on\n",
    "\n",
    "for i in range(k):\n",
    "    model = Sequential()\n",
    "    embed = Embedding(input_dim = vocab_size, output_dim = 20, input_length = X_train_padded.shape[1])\n",
    "\n",
    "    model.add(embed)\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Bidirectional(LSTM(200, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(AttentionWithContext())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    #model.summary()\n",
    "\n",
    "    print(\"Fold :\", numfold)\n",
    "    numfold += 1\n",
    "    lenght = len(X_train_padded)\n",
    "    start = i*lenght//k #gets the starting index for each fold\n",
    "    end = (i+1)*lenght//k #gets the ending index for each fold\n",
    "    \n",
    "    val_padded_fold = X_train_padded[start:end]#splits the data into a k size pice to validatoin\n",
    "    \n",
    "    train_padded_fold = list(X_train_padded[:start])\n",
    "    for e in X_train_padded[end:]:#splits the data into a everything but the k size pice to train\n",
    "        train_padded_fold.append(e)\n",
    "        \n",
    "        \n",
    "    val_label_2_fold = train_label_2[start:end]#splits the data labes into a k size pice to validatoin\n",
    "    train_label_2_fold = list(train_label_2[:start])\n",
    "    for e in train_label_2[end:]:\n",
    "        train_label_2_fold.append(e)\n",
    "    print(len(train_padded_fold),len(train_label_2_fold))\n",
    "    \n",
    "    \n",
    "    #X_train_final2, X_val, y_train_final2, y_val = train_test_split(X_train_padded_fold, train_label_2_fold, test_size = 0.2, shuffle=True)\n",
    "    \n",
    "    X_train_final2 = np.array(train_padded_fold)\n",
    "    y_train_final2 = np.array(train_label_2_fold)\n",
    "    \n",
    "    X_val = np.array(val_padded_fold)\n",
    "    y_val = np.array(val_label_2_fold)\n",
    "    \n",
    "    history = model.fit(X_train_final2, y_train_final2, epochs = 50, batch_size = batch_size, verbose = 1, validation_data = (X_val, y_val), callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 250, 50)\n",
      "(None, 250, 1)\n",
      "Fold : 0\n",
      "16667 16667\n",
      "Epoch 1/50\n",
      "(None, 250, 50)\n",
      "(None, 250, 1)\n",
      "(None, 250, 50)\n",
      "(None, 250, 1)\n",
      "521/521 [==============================] - ETA: 0s - loss: 0.6923 - accuracy: 0.5094(None, 250, 50)\n",
      "(None, 250, 1)\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.6923 - accuracy: 0.5094 - val_loss: 0.6892 - val_accuracy: 0.5025\n",
      "Epoch 2/50\n",
      "521/521 [==============================] - 27s 52ms/step - loss: 0.6807 - accuracy: 0.5637 - val_loss: 0.6722 - val_accuracy: 0.5925\n",
      "Epoch 3/50\n",
      "521/521 [==============================] - 27s 53ms/step - loss: 0.6775 - accuracy: 0.5772 - val_loss: 0.6702 - val_accuracy: 0.5950\n",
      "Epoch 4/50\n",
      "521/521 [==============================] - 27s 53ms/step - loss: 0.6726 - accuracy: 0.5883 - val_loss: 0.6737 - val_accuracy: 0.5739\n",
      "Epoch 5/50\n",
      "521/521 [==============================] - 27s 53ms/step - loss: 0.6704 - accuracy: 0.5898 - val_loss: 0.6682 - val_accuracy: 0.5915\n",
      "Epoch 6/50\n",
      "521/521 [==============================] - 27s 53ms/step - loss: 0.6668 - accuracy: 0.5934 - val_loss: 0.6583 - val_accuracy: 0.6075\n",
      "Epoch 7/50\n",
      "521/521 [==============================] - 27s 53ms/step - loss: 0.6635 - accuracy: 0.6021 - val_loss: 0.6556 - val_accuracy: 0.6105\n",
      "Epoch 8/50\n",
      "521/521 [==============================] - 27s 53ms/step - loss: 0.6545 - accuracy: 0.6102 - val_loss: 0.6627 - val_accuracy: 0.5938\n",
      "Epoch 9/50\n",
      "521/521 [==============================] - 27s 52ms/step - loss: 0.6506 - accuracy: 0.6208 - val_loss: 0.6369 - val_accuracy: 0.6411\n",
      "Epoch 10/50\n",
      "521/521 [==============================] - 27s 53ms/step - loss: 0.6458 - accuracy: 0.6259 - val_loss: 0.6488 - val_accuracy: 0.6318\n",
      "Epoch 11/50\n",
      "521/521 [==============================] - 27s 53ms/step - loss: 0.6364 - accuracy: 0.6348 - val_loss: 0.6217 - val_accuracy: 0.6525\n",
      "Epoch 12/50\n",
      "521/521 [==============================] - 27s 53ms/step - loss: 0.6265 - accuracy: 0.6494 - val_loss: 0.6107 - val_accuracy: 0.6580\n",
      "Epoch 13/50\n",
      "521/521 [==============================] - 27s 53ms/step - loss: 0.6205 - accuracy: 0.6509 - val_loss: 0.6010 - val_accuracy: 0.6669\n",
      "Epoch 14/50\n",
      "521/521 [==============================] - 27s 53ms/step - loss: 0.6094 - accuracy: 0.6595 - val_loss: 0.5888 - val_accuracy: 0.6756\n",
      "Epoch 15/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5983 - accuracy: 0.6666 - val_loss: 0.5803 - val_accuracy: 0.6834\n",
      "Epoch 16/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5885 - accuracy: 0.6782 - val_loss: 0.5759 - val_accuracy: 0.6887\n",
      "Epoch 17/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5786 - accuracy: 0.6900 - val_loss: 0.5758 - val_accuracy: 0.6831\n",
      "Epoch 18/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5729 - accuracy: 0.6919 - val_loss: 0.5710 - val_accuracy: 0.6865\n",
      "Epoch 19/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5657 - accuracy: 0.6953 - val_loss: 0.5587 - val_accuracy: 0.6934\n",
      "Epoch 20/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5606 - accuracy: 0.6971 - val_loss: 0.5573 - val_accuracy: 0.6969\n",
      "Epoch 21/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5503 - accuracy: 0.7082 - val_loss: 0.5465 - val_accuracy: 0.7030\n",
      "Epoch 22/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5423 - accuracy: 0.7160 - val_loss: 0.5752 - val_accuracy: 0.6947\n",
      "Epoch 23/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5382 - accuracy: 0.7152 - val_loss: 0.5423 - val_accuracy: 0.7072\n",
      "Epoch 24/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5269 - accuracy: 0.7259 - val_loss: 0.5274 - val_accuracy: 0.7213\n",
      "Epoch 25/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5202 - accuracy: 0.7322 - val_loss: 0.5277 - val_accuracy: 0.7248\n",
      "Epoch 26/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5188 - accuracy: 0.7317 - val_loss: 0.5181 - val_accuracy: 0.7237\n",
      "Epoch 27/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.5082 - accuracy: 0.7405 - val_loss: 0.5154 - val_accuracy: 0.7281\n",
      "Epoch 28/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.5009 - accuracy: 0.7460 - val_loss: 0.5178 - val_accuracy: 0.7297\n",
      "Epoch 29/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.4918 - accuracy: 0.7519 - val_loss: 0.5174 - val_accuracy: 0.7349\n",
      "Epoch 30/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.4842 - accuracy: 0.7583 - val_loss: 0.5125 - val_accuracy: 0.7343\n",
      "Epoch 31/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.4763 - accuracy: 0.7610 - val_loss: 0.5074 - val_accuracy: 0.7415\n",
      "Epoch 32/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.4681 - accuracy: 0.7702 - val_loss: 0.5068 - val_accuracy: 0.7366\n",
      "Epoch 33/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.4589 - accuracy: 0.7749 - val_loss: 0.5280 - val_accuracy: 0.7383\n",
      "Epoch 34/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.4582 - accuracy: 0.7763 - val_loss: 0.5063 - val_accuracy: 0.7469\n",
      "Epoch 35/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.4467 - accuracy: 0.7833 - val_loss: 0.5053 - val_accuracy: 0.7488\n",
      "Epoch 36/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.4375 - accuracy: 0.7908 - val_loss: 0.4952 - val_accuracy: 0.7506\n",
      "Epoch 37/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.4311 - accuracy: 0.7959 - val_loss: 0.5074 - val_accuracy: 0.7499\n",
      "Epoch 38/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.4246 - accuracy: 0.7972 - val_loss: 0.5526 - val_accuracy: 0.7302\n",
      "Epoch 39/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.4165 - accuracy: 0.8012 - val_loss: 0.5008 - val_accuracy: 0.7545\n",
      "Epoch 40/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.4074 - accuracy: 0.8084 - val_loss: 0.5308 - val_accuracy: 0.7485\n",
      "Epoch 41/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.3988 - accuracy: 0.8130 - val_loss: 0.5146 - val_accuracy: 0.7545\n",
      "Epoch 00041: early stopping\n",
      "(None, 250, 50)\n",
      "(None, 250, 1)\n",
      "Fold : 1\n",
      "16667 16667\n",
      "Epoch 1/50\n",
      "(None, 250, 50)\n",
      "(None, 250, 1)\n",
      "(None, 250, 50)\n",
      "(None, 250, 1)\n",
      "521/521 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4979(None, 250, 50)\n",
      "(None, 250, 1)\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.6936 - accuracy: 0.4979 - val_loss: 0.6996 - val_accuracy: 0.4975\n",
      "Epoch 2/50\n",
      "521/521 [==============================] - 27s 52ms/step - loss: 0.6880 - accuracy: 0.5354 - val_loss: 0.6776 - val_accuracy: 0.5909\n",
      "Epoch 3/50\n",
      "521/521 [==============================] - 27s 52ms/step - loss: 0.6783 - accuracy: 0.5712 - val_loss: 0.6678 - val_accuracy: 0.5907\n",
      "Epoch 4/50\n",
      "521/521 [==============================] - 27s 52ms/step - loss: 0.6745 - accuracy: 0.5777 - val_loss: 0.6663 - val_accuracy: 0.5919\n",
      "Epoch 5/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.6709 - accuracy: 0.5897 - val_loss: 0.6655 - val_accuracy: 0.5941\n",
      "Epoch 6/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.6685 - accuracy: 0.5918 - val_loss: 0.6756 - val_accuracy: 0.5634\n",
      "Epoch 7/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.6632 - accuracy: 0.6007 - val_loss: 0.6612 - val_accuracy: 0.6009\n",
      "Epoch 8/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.6545 - accuracy: 0.6155 - val_loss: 0.6396 - val_accuracy: 0.6322\n",
      "Epoch 9/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.6480 - accuracy: 0.6245 - val_loss: 0.6329 - val_accuracy: 0.6395\n",
      "Epoch 10/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.6413 - accuracy: 0.6318 - val_loss: 0.6295 - val_accuracy: 0.6455\n",
      "Epoch 11/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.6360 - accuracy: 0.6399 - val_loss: 0.6211 - val_accuracy: 0.6562\n",
      "Epoch 12/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.6320 - accuracy: 0.6399 - val_loss: 0.6105 - val_accuracy: 0.6671\n",
      "Epoch 13/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.6207 - accuracy: 0.6580 - val_loss: 0.6063 - val_accuracy: 0.6703\n",
      "Epoch 14/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.6131 - accuracy: 0.6619 - val_loss: 0.6027 - val_accuracy: 0.6767\n",
      "Epoch 15/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.6063 - accuracy: 0.6707 - val_loss: 0.5816 - val_accuracy: 0.6891\n",
      "Epoch 16/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5934 - accuracy: 0.6850 - val_loss: 0.5877 - val_accuracy: 0.6778\n",
      "Epoch 17/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5838 - accuracy: 0.6856 - val_loss: 0.5647 - val_accuracy: 0.7085\n",
      "Epoch 18/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.5789 - accuracy: 0.6887 - val_loss: 0.5584 - val_accuracy: 0.7109\n",
      "Epoch 19/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.5672 - accuracy: 0.6994 - val_loss: 0.5486 - val_accuracy: 0.7132\n",
      "Epoch 20/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.5587 - accuracy: 0.7064 - val_loss: 0.5412 - val_accuracy: 0.7158\n",
      "Epoch 21/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.5528 - accuracy: 0.7075 - val_loss: 0.5484 - val_accuracy: 0.7024\n",
      "Epoch 22/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5408 - accuracy: 0.7175 - val_loss: 0.5393 - val_accuracy: 0.7121\n",
      "Epoch 23/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5336 - accuracy: 0.7235 - val_loss: 0.5282 - val_accuracy: 0.7283\n",
      "Epoch 24/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5274 - accuracy: 0.7206 - val_loss: 0.5152 - val_accuracy: 0.7296\n",
      "Epoch 25/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5206 - accuracy: 0.7330 - val_loss: 0.5138 - val_accuracy: 0.7368\n",
      "Epoch 26/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.5131 - accuracy: 0.7382 - val_loss: 0.5064 - val_accuracy: 0.7423\n",
      "Epoch 27/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.5050 - accuracy: 0.7437 - val_loss: 0.5039 - val_accuracy: 0.7387\n",
      "Epoch 28/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.4968 - accuracy: 0.7471 - val_loss: 0.4940 - val_accuracy: 0.7444\n",
      "Epoch 29/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.4953 - accuracy: 0.7496 - val_loss: 0.5051 - val_accuracy: 0.7420\n",
      "Epoch 30/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.4853 - accuracy: 0.7539 - val_loss: 0.5145 - val_accuracy: 0.7306\n",
      "Epoch 31/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.4826 - accuracy: 0.7559 - val_loss: 0.4935 - val_accuracy: 0.7497\n",
      "Epoch 32/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.4698 - accuracy: 0.7686 - val_loss: 0.4997 - val_accuracy: 0.7517\n",
      "Epoch 33/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.4660 - accuracy: 0.7699 - val_loss: 0.4861 - val_accuracy: 0.7566\n",
      "Epoch 34/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.4589 - accuracy: 0.7735 - val_loss: 0.4801 - val_accuracy: 0.7612\n",
      "Epoch 35/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.4497 - accuracy: 0.7807 - val_loss: 0.4843 - val_accuracy: 0.7605\n",
      "Epoch 36/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.4404 - accuracy: 0.7877 - val_loss: 0.5013 - val_accuracy: 0.7618\n",
      "Epoch 37/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.4328 - accuracy: 0.7926 - val_loss: 0.4891 - val_accuracy: 0.7622\n",
      "Epoch 38/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.4271 - accuracy: 0.7958 - val_loss: 0.4984 - val_accuracy: 0.7495\n",
      "Epoch 39/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.4190 - accuracy: 0.8038 - val_loss: 0.4902 - val_accuracy: 0.7583\n",
      "Epoch 00039: early stopping\n",
      "(None, 250, 50)\n",
      "(None, 250, 1)\n",
      "Fold : 2\n",
      "16666 16666\n",
      "Epoch 1/50\n",
      "(None, 250, 50)\n",
      "(None, 250, 1)\n",
      "(None, 250, 50)\n",
      "(None, 250, 1)\n",
      "520/521 [============================>.] - ETA: 0s - loss: 0.6937 - accuracy: 0.5067(None, 250, 50)\n",
      "(None, 250, 1)\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.6938 - accuracy: 0.5066 - val_loss: 0.6963 - val_accuracy: 0.5032\n",
      "Epoch 2/50\n",
      "521/521 [==============================] - 27s 52ms/step - loss: 0.6871 - accuracy: 0.5466 - val_loss: 0.6809 - val_accuracy: 0.5703\n",
      "Epoch 3/50\n",
      "521/521 [==============================] - 27s 53ms/step - loss: 0.6761 - accuracy: 0.5808 - val_loss: 0.6708 - val_accuracy: 0.5836\n",
      "Epoch 4/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.6718 - accuracy: 0.5861 - val_loss: 0.6690 - val_accuracy: 0.5871\n",
      "Epoch 5/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.6694 - accuracy: 0.5931 - val_loss: 0.6651 - val_accuracy: 0.5944\n",
      "Epoch 6/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.6639 - accuracy: 0.6012 - val_loss: 0.6617 - val_accuracy: 0.5961\n",
      "Epoch 7/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.6557 - accuracy: 0.6147 - val_loss: 0.6639 - val_accuracy: 0.5959\n",
      "Epoch 8/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.6500 - accuracy: 0.6231 - val_loss: 0.6440 - val_accuracy: 0.6212\n",
      "Epoch 9/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.6448 - accuracy: 0.6283 - val_loss: 0.6399 - val_accuracy: 0.6313\n",
      "Epoch 10/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.6373 - accuracy: 0.6411 - val_loss: 0.6367 - val_accuracy: 0.6409\n",
      "Epoch 11/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.6311 - accuracy: 0.6439 - val_loss: 0.6217 - val_accuracy: 0.6502\n",
      "Epoch 12/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.6224 - accuracy: 0.6552 - val_loss: 0.6151 - val_accuracy: 0.6572\n",
      "Epoch 13/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.6147 - accuracy: 0.6594 - val_loss: 0.6087 - val_accuracy: 0.6579\n",
      "Epoch 14/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.6019 - accuracy: 0.6712 - val_loss: 0.5933 - val_accuracy: 0.6724\n",
      "Epoch 15/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5952 - accuracy: 0.6734 - val_loss: 0.6028 - val_accuracy: 0.6573\n",
      "Epoch 16/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5851 - accuracy: 0.6823 - val_loss: 0.5764 - val_accuracy: 0.6853\n",
      "Epoch 17/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5760 - accuracy: 0.6886 - val_loss: 0.5682 - val_accuracy: 0.6910\n",
      "Epoch 18/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.5707 - accuracy: 0.6916 - val_loss: 0.5580 - val_accuracy: 0.7033\n",
      "Epoch 19/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5609 - accuracy: 0.6987 - val_loss: 0.5508 - val_accuracy: 0.7025\n",
      "Epoch 20/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5535 - accuracy: 0.7038 - val_loss: 0.5456 - val_accuracy: 0.7046\n",
      "Epoch 21/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5456 - accuracy: 0.7098 - val_loss: 0.5403 - val_accuracy: 0.7112\n",
      "Epoch 22/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.5330 - accuracy: 0.7166 - val_loss: 0.5269 - val_accuracy: 0.7203\n",
      "Epoch 23/50\n",
      "521/521 [==============================] - 28s 53ms/step - loss: 0.5271 - accuracy: 0.7252 - val_loss: 0.5222 - val_accuracy: 0.7252\n",
      "Epoch 24/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.5230 - accuracy: 0.7281 - val_loss: 0.5179 - val_accuracy: 0.7289\n",
      "Epoch 25/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.5136 - accuracy: 0.7378 - val_loss: 0.5172 - val_accuracy: 0.7311\n",
      "Epoch 26/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.5086 - accuracy: 0.7386 - val_loss: 0.5160 - val_accuracy: 0.7327\n",
      "Epoch 27/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.5053 - accuracy: 0.7428 - val_loss: 0.5093 - val_accuracy: 0.7370\n",
      "Epoch 28/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.4943 - accuracy: 0.7479 - val_loss: 0.5074 - val_accuracy: 0.7381\n",
      "Epoch 29/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.4896 - accuracy: 0.7520 - val_loss: 0.5411 - val_accuracy: 0.7059\n",
      "Epoch 30/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.4801 - accuracy: 0.7584 - val_loss: 0.5165 - val_accuracy: 0.7293\n",
      "Epoch 31/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.4723 - accuracy: 0.7667 - val_loss: 0.5045 - val_accuracy: 0.7478\n",
      "Epoch 32/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.4668 - accuracy: 0.7682 - val_loss: 0.5067 - val_accuracy: 0.7447\n",
      "Epoch 33/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.4582 - accuracy: 0.7724 - val_loss: 0.5055 - val_accuracy: 0.7474\n",
      "Epoch 34/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.4516 - accuracy: 0.7820 - val_loss: 0.5099 - val_accuracy: 0.7411\n",
      "Epoch 35/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.4403 - accuracy: 0.7882 - val_loss: 0.4953 - val_accuracy: 0.7483\n",
      "Epoch 36/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.4363 - accuracy: 0.7890 - val_loss: 0.5164 - val_accuracy: 0.7442\n",
      "Epoch 37/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.4269 - accuracy: 0.7989 - val_loss: 0.5063 - val_accuracy: 0.7436\n",
      "Epoch 38/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.4176 - accuracy: 0.8024 - val_loss: 0.5081 - val_accuracy: 0.7539\n",
      "Epoch 39/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.4144 - accuracy: 0.8035 - val_loss: 0.5114 - val_accuracy: 0.7563\n",
      "Epoch 40/50\n",
      "521/521 [==============================] - 28s 54ms/step - loss: 0.4048 - accuracy: 0.8099 - val_loss: 0.5091 - val_accuracy: 0.7540\n",
      "Epoch 00040: early stopping\n"
     ]
    }
   ],
   "source": [
    "#k-fold cross validation\n",
    "#model 2\n",
    "k = 3 #number of folds\n",
    "numfold = 0 #what fold we are on\n",
    "\n",
    "for i in range(k):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=20, input_length=X_train_padded.shape[1]))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Bidirectional(LSTM(50, return_sequences=True)))\n",
    "    model.add(LSTM(100, return_sequences=True))\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(AttentionWithContext())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "\n",
    "    print(\"Fold :\", numfold)\n",
    "    numfold += 1\n",
    "    lenght = len(X_train_padded)\n",
    "    start = i*lenght//k #gets the starting index for each fold\n",
    "    end = (i+1)*lenght//k #gets the ending index for each fold\n",
    "    \n",
    "    val_padded_fold = X_train_padded[start:end]#splits the data into a k size pice to validatoin\n",
    "    \n",
    "    train_padded_fold = list(X_train_padded[:start])\n",
    "    for e in X_train_padded[end:]:#splits the data into a everything but the k size pice to train\n",
    "        train_padded_fold.append(e)\n",
    "        \n",
    "        \n",
    "    val_label_2_fold = train_label_2[start:end]#splits the data labes into a k size pice to validatoin\n",
    "    train_label_2_fold = list(train_label_2[:start])\n",
    "    for e in train_label_2[end:]:\n",
    "        train_label_2_fold.append(e)\n",
    "    print(len(train_padded_fold),len(train_label_2_fold))\n",
    "    \n",
    "    \n",
    "    #X_train_final2, X_val, y_train_final2, y_val = train_test_split(X_train_padded_fold, train_label_2_fold, test_size = 0.2, shuffle=True)\n",
    "    \n",
    "    X_train_final2 = np.array(train_padded_fold)\n",
    "    y_train_final2 = np.array(train_label_2_fold)\n",
    "    \n",
    "    X_val = np.array(val_padded_fold)\n",
    "    y_val = np.array(val_label_2_fold)\n",
    "    \n",
    "    history = model.fit(X_train_final2, y_train_final2, epochs = 50, batch_size = batch_size, verbose = 1, validation_data = (X_val, y_val), callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 250, 400)\n",
      "(None, 250, 1)\n",
      "Fold : 0\n",
      "16667 16667\n",
      "Epoch 1/50\n",
      "(None, 250, 400)\n",
      "(None, 250, 1)\n",
      "(None, 250, 400)\n",
      "(None, 250, 1)\n",
      "521/521 [==============================] - ETA: 0s - loss: 0.6926 - accuracy: 0.5122(None, 250, 400)\n",
      "(None, 250, 1)\n",
      "521/521 [==============================] - 22s 42ms/step - loss: 0.6926 - accuracy: 0.5122 - val_loss: 0.6869 - val_accuracy: 0.5715\n",
      "Epoch 2/50\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.6793 - accuracy: 0.5708 - val_loss: 0.6753 - val_accuracy: 0.5790\n",
      "Epoch 3/50\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.6730 - accuracy: 0.5805 - val_loss: 0.6697 - val_accuracy: 0.5892\n",
      "Epoch 4/50\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.6645 - accuracy: 0.5957 - val_loss: 0.6558 - val_accuracy: 0.5999\n",
      "Epoch 5/50\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.6472 - accuracy: 0.6138 - val_loss: 0.6339 - val_accuracy: 0.6347\n",
      "Epoch 6/50\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.6415 - accuracy: 0.6162 - val_loss: 0.6297 - val_accuracy: 0.6360\n",
      "Epoch 7/50\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.6345 - accuracy: 0.6275 - val_loss: 0.6228 - val_accuracy: 0.6449\n",
      "Epoch 8/50\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.6221 - accuracy: 0.6430 - val_loss: 0.6103 - val_accuracy: 0.6513\n",
      "Epoch 9/50\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.6143 - accuracy: 0.6469 - val_loss: 0.6025 - val_accuracy: 0.6607\n",
      "Epoch 10/50\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.6106 - accuracy: 0.6563 - val_loss: 0.5957 - val_accuracy: 0.6690\n",
      "Epoch 11/50\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.5997 - accuracy: 0.6665 - val_loss: 0.5858 - val_accuracy: 0.6833\n",
      "Epoch 12/50\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.5890 - accuracy: 0.6719 - val_loss: 0.5807 - val_accuracy: 0.6803\n",
      "Epoch 13/50\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.5813 - accuracy: 0.6822 - val_loss: 0.5595 - val_accuracy: 0.7039\n",
      "Epoch 14/50\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.5694 - accuracy: 0.6901 - val_loss: 0.5564 - val_accuracy: 0.7031\n",
      "Epoch 15/50\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.5638 - accuracy: 0.6957 - val_loss: 0.5457 - val_accuracy: 0.7072\n",
      "Epoch 16/50\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.5518 - accuracy: 0.7055 - val_loss: 0.5349 - val_accuracy: 0.7128\n",
      "Epoch 17/50\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.5419 - accuracy: 0.7124 - val_loss: 0.5255 - val_accuracy: 0.7240\n",
      "Epoch 18/50\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.5293 - accuracy: 0.7214 - val_loss: 0.5215 - val_accuracy: 0.7295\n",
      "Epoch 19/50\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.5193 - accuracy: 0.7281 - val_loss: 0.5201 - val_accuracy: 0.7245\n",
      "Epoch 20/50\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.5102 - accuracy: 0.7341 - val_loss: 0.5197 - val_accuracy: 0.7309\n",
      "Epoch 21/50\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.5003 - accuracy: 0.7422 - val_loss: 0.5063 - val_accuracy: 0.7367\n",
      "Epoch 22/50\n",
      "433/521 [=======================>......] - ETA: 3s - loss: 0.4926 - accuracy: 0.7475"
     ]
    }
   ],
   "source": [
    "#model 3\n",
    "#k-fold cross validation\n",
    "k = 3 #number of folds\n",
    "numfold = 0 #what fold we are on\n",
    "\n",
    "for i in range(k):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=20, input_length=X_train_padded.shape[1]))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Bidirectional(GRU(200, return_sequences=True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(AttentionWithContext())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    print(\"Fold :\", numfold)\n",
    "    numfold += 1\n",
    "    lenght = len(X_train_padded)\n",
    "    start = i*lenght//k #gets the starting index for each fold\n",
    "    end = (i+1)*lenght//k #gets the ending index for each fold\n",
    "    \n",
    "    val_padded_fold = X_train_padded[start:end]#splits the data into a k size pice to validatoin\n",
    "    \n",
    "    train_padded_fold = list(X_train_padded[:start])\n",
    "    for e in X_train_padded[end:]:#splits the data into a everything but the k size pice to train\n",
    "        train_padded_fold.append(e)\n",
    "        \n",
    "        \n",
    "    val_label_2_fold = train_label_2[start:end]#splits the data labes into a k size pice to validatoin\n",
    "    train_label_2_fold = list(train_label_2[:start])\n",
    "    for e in train_label_2[end:]:\n",
    "        train_label_2_fold.append(e)\n",
    "    print(len(train_padded_fold),len(train_label_2_fold))\n",
    "    \n",
    "    \n",
    "    #X_train_final2, X_val, y_train_final2, y_val = train_test_split(X_train_padded_fold, train_label_2_fold, test_size = 0.2, shuffle=True)\n",
    "    \n",
    "    X_train_final2 = np.array(train_padded_fold)\n",
    "    y_train_final2 = np.array(train_label_2_fold)\n",
    "    \n",
    "    X_val = np.array(val_padded_fold)\n",
    "    y_val = np.array(val_label_2_fold)\n",
    "    \n",
    "    history = model.fit(X_train_final2, y_train_final2, epochs = 50, batch_size = batch_size, verbose = 1, validation_data = (X_val, y_val), callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross validation\n",
    "#model 4\n",
    "k = 3 #number of folds\n",
    "numfold = 0 #what fold we are on\n",
    "\n",
    "for i in range(k):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=20, input_length=input_length))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Bidirectional(GRU(50, return_sequences=True)))\n",
    "    model.add(Bidirectional(GRU(100, return_sequences=True)))\n",
    "    model.add(Bidirectional(GRU(50, return_sequences=True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Attention())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    #model.summary()\n",
    "\n",
    "    print(\"Fold :\", numfold)\n",
    "    numfold += 1\n",
    "    lenght = len(X_train_padded)\n",
    "    start = i*lenght//k #gets the starting index for each fold\n",
    "    end = (i+1)*lenght//k #gets the ending index for each fold\n",
    "    \n",
    "    val_padded_fold = X_train_padded[start:end]#splits the data into a k size pice to validatoin\n",
    "    \n",
    "    train_padded_fold = list(X_train_padded[:start])\n",
    "    for e in X_train_padded[end:]:#splits the data into a everything but the k size pice to train\n",
    "        train_padded_fold.append(e)\n",
    "        \n",
    "        \n",
    "    val_label_2_fold = train_label_2[start:end]#splits the data labes into a k size pice to validatoin\n",
    "    train_label_2_fold = list(train_label_2[:start])\n",
    "    for e in train_label_2[end:]:\n",
    "        train_label_2_fold.append(e)\n",
    "    print(len(train_padded_fold),len(train_label_2_fold))\n",
    "    \n",
    "    \n",
    "    #X_train_final2, X_val, y_train_final2, y_val = train_test_split(X_train_padded_fold, train_label_2_fold, test_size = 0.2, shuffle=True)\n",
    "    \n",
    "    X_train_final2 = np.array(train_padded_fold)\n",
    "    y_train_final2 = np.array(train_label_2_fold)\n",
    "    \n",
    "    X_val = np.array(val_padded_fold)\n",
    "    y_val = np.array(val_label_2_fold)\n",
    "    \n",
    "    history = model.fit(X_train_final2, y_train_final2, epochs = 50, batch_size = batch_size, verbose = 1, validation_data = (X_val, y_val), callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come.\"\n",
      "0\n",
      "b'I saw this movie many years ago, and just for kicks decided to rent it and watch it again. The plot is a carbon copy from Fright Night. I did like the hairy vampire and the bug eating driver. Otherwise it was not good at all.'\n",
      "1\n",
      "b\"Idea is great, spoiled big time by the judges.<br /><br />Why make fun of people? if what the inventors say is true, and as most of them say, they spent their life saving on the invention, the minimum is to reject the idea without making fun out of the people.<br /><br />also, it shows when they want to accept an idea by the crier that they added to the judges.<br /><br />The only one i respect out of the judges is the one who always sits on the right of the table, he is a respectable person<br /><br />of course the English snob who claims to be a business man, wearing a suite doesn't make you one pal<br /><br />last but not least, the big guy who sits between the English and the crier. wake up man, the is no job called and inventor for you to call yourself one. an inventor is an attribute not a job man.<br /><br />i think they wanted to add someone like Simon from the American idol, they thought it worked there, it can work here as well. the context is different and the idea is totally different.<br /><br />it is a good idea and they could have done a good show out of it if they just change the judges and remove their act and attitude.<br /><br />just stop making fun of the people.\"\n",
      "2\n",
      "b\"I watched this movie last night, i'm a huge fan of the book, and i was pretty happy with the version in which Winona Ryder and Susan Sarandon starred. But this one, it's just awful. Oh my God, i don't understand how they dared to ripped apart this classic story and made the characters totally different, starting with the switching of Beth being the younger sister, and making Amy the 3rd one. And Jo interpretation, terrible, Jo was a feminist, intelligent and kinda angry young lady, and the actress portraying Jo in this movie acts like a foolish and very annoying little girl. And what's with the Laurie going to war?. i'm OK with the fact that when a book is made into a movie there has to be some changes made, but not re-write the whole story. very very bad done.\"\n",
      "3\n",
      "b\"I was very happy and at the same time quite surprised by other positive comments written by non-Koreans below. This movie is amazingly heartshaking, and shows very 'sad but warm' view toward life which is typical to Korean people. I thought other foreigners would not understand this delicate feeling and under-rate this quiet film as a boring one, but I was wrong. The attraction of this film might be hard to avoid to foreigners, too. (Even without subtitle...)<br /><br />I would like to mention some points others have missed. Of course, this film depicts love between a man and a woman. However, the very theme is way beyond that. Actually, it is about time, value of remembrance, and death. In this film, the focus is not on the 'love affair' between two people. As some pointed out, they do not kiss, they do not hug each other, even without holding hands. So love itself is not completed (whether positively or negatively) in the film. Rather, what haunts Jungwon (a leading actor) is his impending death. He's running out of time, he can't hold it, leaving a few behind including his father and of course, Darim (a metermaid). So the problem is how he can face the death and leave something valuable in his short life, not how he can make love with Darim.<br /><br />This kind of theme sounds very familiar to us. There are lots of movies regarding patients with uncurable disease such as 'Love Story'. However, what makes this film outstanding is the way Jungwon deals his death. He is a loser, but tried to do his best while he's alive, IN A SILENT WAY. He does not tell anybody around him about his death. He hides something in his mind but without rage, hate, vengeance. He just tried to do best while he was alive. This limited communication and obedience to fate is the typical mindset of Koreans and the point most Western people don't understand or at best, misunderstand.<br /><br />This theme is very effectively expressed by the director of this film (surprisingly, his debut). Some say he's much influenced by Japanese director Ozu Yasujiro, who directed Tokyo Story. Indeed, I remember I read in some magazine that the director himself admitted he was influenced by Ozu. I'm not that knowledged to analyze his style comparing to Ozu's, but they have some in common and some not. Low angle and static camera, especially remind us Ozu's style. But, in terms of theme again, this Korean director seems to have somewhat warmer and hopeful vision.<br /><br />It is expressed concisely with Jungwon's last photograph. Very well done and really heartbreaking scene, I think. Actually, the director first had the idea of this film when he participated the funeral of a very famous Korean folk singer who died young of mysterious suicide. They say he saw the photograph of the singer at the funeral and thought of a film on death and remembrance. (And possibly hope for the remnants, I think...)<br /><br />I highly recommend this film to anybody who has deep interest in film art as well as Korean culture. This film, in my opinion, can be rivaled with other movies like Tokyo story, and a sort of American Beauty. It is that great if without language barrier. DVD version is going to be out in the market this February, so it might be a little help for foreigners with English subtitle.\"\n",
      "4\n",
      "b\"I had always eyed Italian horror maestro Dario Argento's efforts as producer with a certain suspicion and these were only confirmed after my fairly recent viewing of Lamberto Bava's terrible DEMONS (1985); the fact that this was supposed to be its third installment did not sound promising at all but I decided to give the film a rental regardless now that we're in full Halloween swing. I checked out the theatrical trailer on the Anchor Bay DVD prior to viewing the main feature \\xc2\\x96 the undeniably striking visuals had me intrigued to be sure but, then, the film proper (which makes no more sense than what's presented in that frenzied two-minute montage and, in retrospect, can be seen to have wisely compiled most of its highlights) proved a definite let-down!<br /><br />Opening promisingly enough with a medieval prologue straight out of Alexander NEVSKY (1938), it goes downhill fast because it relies too much on surreal imagery at the expense of narrative. Consequently, several characters randomly take center-stage throughout \\xc2\\x96 with the insufferable male lead succumbing to the dark forces early on, the sinister-looking Bishop (Feodor Chaliapin) resulting a mere red herring, the mysterious black priest gradually assuming heroic qualities, the leading lady is for whatever reason preyed upon by a goat-shaped demon (culminating in a sexual rite conducted in front of the other cultists lifted all-too-obviously from ROSEMARY'S BABY [1968]) and a reasonably impressive 13-year old Asia Argento as the rebellious but likable sacristan's daughter (who emerges as the only survivor by the end). Incidentally, the older Argento also co-wrote the film's story and screenplay along with director Soavi and (under a pseudonym after they apparently fell out with Dario in the early stages of production) original helmer Lamberto Bava and prolific genre scribe Dardano Sacchetti (whom I met at the 61st Venice Film Festival in 2004).<br /><br />The extremely muddled second half of the film, then, sees a group of people \\xc2\\x96 including the inevitable teenagers but also a doddering English couple (whose constant bickering is given an amusingly nasty punchline) \\xc2\\x96 similarly shut inside a building in the grip of evil spirits (the church being the burial ground of a satanic cult)\\xc2\\x85not that this horror outing is likely to dispel memories of Luis Bunuel's sublimely surreal THE EXTERMINATING ANGEL (1962) you see! In the end, the film is all the more disappointing (though Sergio Stivaletti's gruesome effects, at least, are notable) given that I had thoroughly enjoyed the only other Soavi title I'd watched \\xc2\\x96 CEMETERY MAN (1994), which I own via the R2 SE DVD. That said, I'd still like to catch his debut feature \\xc2\\x96 STAGE FRIGHT (1987) \\xc2\\x96 and the director's follow-up effort to THE CHURCH, entitled THE SECT (1991)...\"\n",
      "5\n",
      "b\"I saw this in a theatre back in 1982. I expected a stupid T&A movie. That's not what I got.<br /><br />It's basically about three teenage boys trying to have sex. We get the expected sex jokes and scenes--but, for once, they're actually pretty funny!<br /><br />Yeah, they're stupid but I enjoyed them anyways. Also, there was a surprising amount of male nudity.<br /><br />Then the movie, about halfway through, takes a sudden dramatic turn as one of the boys (winningly played by Lawrence Monoson) falls in love with a girl. Then the jokes stop and things get very dark. I'm not going to give away what happens but I was very surprised at the sudden turn in events. The movie brings up some very important subjects and treats them realistically and with intelligence. And it has a real heart-breaking ending.<br /><br />I'm giving this a 10 because this is probably one of the best teen sex comedy/dramas ever made. It mixes fantasy and realism together and works! What more can you ask for? Also it has a GREAT soundtrack.\"\n",
      "6\n",
      "b'I watched the first show of each series just to see and what a waste of time. The girl from Emmerdale she was fat so yeah she should be in fat friend but no one every lost weigh.<br /><br />Like Itv made a big mistake with this.<br /><br />Bad Girls is 100times better.<br /><br />I feel that the whole show was just about large people trying to loose weight but never did then they tried to have love storyline oh my god what a a waste of time and also air time. This show has not been repeated on ITV2/3/4 yeah thats how good it is.<br /><br />I would say do not by th box sets just a waste of money.<br /><br />BEWARE'\n",
      "7\n",
      "b'Am I the only person who saw and remembers Amadeus. Every scene in \"Copying\" has its counterpart in the Milos Forman, Amadeus - from the galloping carriages accompanied by frenzied strings, to the entrance of Anna through the dark hallway preceded by the man with the key, to the very dialog of the conclusion with Anna at Beethoven\\'s feet a la Salieri before the dying Mozart. Does no one else recall the dialog in that script: <br /><br />Salieri \"Time?\" , Mozart \"Common time\". .....\"We begin with the strings...\" and so it \"copies\".<br /><br />Remember Cynthia Nixon leaning against a door jamb, tears falling down her freckled face - same scene, just replace her with Beethoven\\'s nephew. Even the scatological humor (fart jokes) are the same. We even have a cardinal followed around by a plate of sweet cookies which recalls Salieri in the banquet hall. Does no one else remember? !!!!<br /><br />And the scene where Anna cries to God \"Why did you give me this gift?\"... Salieri said the same thing!<br /><br />And beyond all that we have a juvenile script with an opening exposition that reads like the character identifications you\\'d find in a children\\'s story. \"I am Mr. Beethoven. I am the composer....\"<br /><br />And what, after all, is the purpose of the bridge builder? Other than to juxtapose the technical against the artistic... a comparison that is not at all developed - probably because it has no meaning, especially when it comes to Beethove who was a master of the technical.<br /><br />There\\'s only one great scene - the playing of the ninth. First, the music, which of itself takes you to tears. But then, there is the highly erotic interaction of Anna the copyist keeping time and Beethoven conducting which surpasses the most explicit sexual intimacy. So intense, it\\'s almost embarrassing.<br /><br />But, please - the rest of the film - Where is the world\\'s collective memory? You have seen this all before .... and better!'\n",
      "8\n",
      "b\"Sci-Fi channel thinks this IS Sci-Fi; it's a shame. Big Bugs, Snakes, Mythical Beasties, on and on, they persist.<br /><br />Some one at Universal had the brains to include BattleStar Galactica (the new, good one) and Firefly for a brief moment in their line up. I know they know they difference between total garbage and extremely high quality sci-fi.<br /><br />A few years back they were on about how they were going all high and mighty, making productions that were not just for us mere, lame-o Trekkies. Thanks so much, Sci-Fi! You know, you make movies so bad, even Trekkies won't watch them, so you achieved your goal! Fire Serpent, Ice Spiders, Manticore, Larva etc.and a vast unrelenting crap-storm later, and they're still churning out just faster than the latest flu virus! How they do it is beyond my ken. Why they do it, I just don't know. How they can ignore these reviews, comments, blogs and e-mails, I don't know either, but it's clear they don't think much of their audience or care about our opinions! They seem to think this is what sci-fi fans want! You would think one or two good productions with some sense would creep through when whoever green lights this junk is on vacation. At least they're employing the collection of Misfit Toys; many of the sci-fi movie of the week actors were in Science Fiction shows once and now need the cash. Love you folks, and hope you get some better work!\"\n",
      "9\n",
      "b\"I think this movie is different apart from most films I've seen. It was exciting in a way, and no matter what others say, I say, I was surprised about the final solution. Certainly didn't see it coming!! Although it's sad, it's worth watching.. I can't think of any movie that would be like this! Actors knew what they were doing. If you say this movie sucks, you say probably what most people would say. But, if someone says that this movie is ordinary, I absolutely don't agree. And Norman Reedus should be more noticed.<br /><br />Maybe I'm freak but I liked this very much. It was kind of mess, but who cares? I'm tired of boring and ordinary movies.\"\n",
      "10\n",
      "b'During my teens or should I say prime time I was \"eating up\" all kinds of SF novels every day of the week. It was in the Sixties and Seventies when TV was not such a important leisure time killer like today, one night in the mid seventies I watched the movie on TV I think it was ARD and I was stunned. I was impressed in a way that I can almost remember each scene even today.<br /><br />Nowadays I observe my kids playing the SIMS or something like that and I think we are close to what that Fassbender Movie expressed. I also would highly appreciate if I could buy this movie on a DVD. But in vain I tried almost everything to get a hint where. The movie MATRIX cannot touch by far the quality and the state of art of this movie. And by the way by now we do not have a glue if we were a superior reality or just one of a couple simulation models. Probably after our death we definitely will know...'\n",
      "11\n",
      "b\"Hell to Pay was a disappointment. It did not have anywhere near the substance of a B Western movie, and should in no way be compared to a fantastic movie like Silverado. The dialog was dull, the plot was torpid, the soundtrack was overbearingly unnecessary, and the acting was awful. Even the professionals could've taken some lessons from the Sunset Carson School of Acting. The only positive thing about this movie is that it showcased some of the top Cowboy shooters in the nation, but you can see them in a better light in any SASS video. The packaging of this feature makes it very enticing, and the preview is decent, but it's all over after that.\"\n",
      "12\n",
      "b\"Of all the kids movies I have seen over the years this was probably the worst. I took four kids aged from 7 to 11 and none of them liked it.<br /><br />The script seemed to be based on a Willy Wonka style story but it just didn't have anything to it.<br /><br />If you are considering seeing this movie dont waste your time, it is bad.<br /><br />They are making a sequel, so it may be worth watching to see if they can even make a worse movie, but I don't think it is possible.\"\n",
      "13\n",
      "b'If you took all the stock elements of a Shrek movie (grumpy ogre, annoying donkey, cute kitty, obligatory dance number, etc.), put them in a blender and condensed it to 20 minutes, you\\'d have this mess. Painful to watch; I may have laughed once. The story and dialogue are rushed beyond comprehension, with the voice actors sounding like they phoned in their lines. The final reworked rendition of \"The Christmas Story\" poem felt like it was written by a committee in five minutes. And boy, a little Eddie Murphy goes a long way. With its desperate attempt to be hip and current, this show will be long outdated and forgotten while classics like \"The Grinch\\xc2\\x85\" will remain timeless. A sad waste of effort by all involved, a veritable \"jumping of the shark\" for the Shrek franchise.'\n",
      "14\n",
      "b\"I saw this play on Showtime some years back in the comfort of my home and when the final note was struck, I wanted to jump off the sofa and give the production a standing ovation. As it was, I shed a tear that it was such a bunch of fantastic performances and songs. For my birthday, my kids bought me the VHS version as well as the Cd of the play with Len Cariou in the Sweeny Todd Role. <br /><br />I've shared the play with many...some finding the subject a bit sick, but none having anything but praise for the songs.<br /><br />I've always loved the interplay in songs with Angela Lansbury and George Hearn as well as Hearn and Edmund Lyndeck as Judge Turpin.<br /><br />I must own the DVD.\"\n",
      "15\n",
      "b\"One of the most magnificent movies ever made. The acting of Charles Buchinski (later known as Bronson) is simply outstanding. This is the crown on the career of director Winner, who himself was often quoted saying this was his masterpiece. The plot has been copied many times, but it's never been topped. Wildey J. Moore, the gun manufacturer, many times claimed his brand's growth since the mid 80s can be fully credited to DW3, and rightly so. This is not just a movie, this is art that many generations will admire and appreciate. Although this movie has never been fully appreciated in the USA, it has found a huge following in Europe and Asia, where the movie is regularly shown at film schools and it is still a popular hit in student cinemas all across Europe. All in all, a true classic.\"\n",
      "16\n",
      "b'Pretty.<br /><br />Pretty actresses and actors. Pretty bad script. Pretty frequent \"let\\'s strip to our undies\" scenes. Pretty fair F/X. Pretty jarring location decisions (the college dorm room looks like a high-end hotel room - probably because it was shot at a hotel). Pretty bland storyline. Pretty awful dialog. Pretty locations. Pretty annoying editing, unless you like the music video flash-cut style.<br /><br />This one isn\\'t a guilty pleasure - this is more an embarrassing one. If you must watch this, pick a good dance/techno album and turn the sound off on the movie - you\\'ll see the pretty people in their pretty black undies, and probably follow the story just fine.<br /><br />The cast may be able to act - I doubt that anyone could look skilled given the lines/plot that they had to deal with.'\n",
      "17\n",
      "b'This film is on my top 20 comedies list. This is a truly unique film. To the reviewer who said \"I must be really missing something.\"--you are correct. You are missing something. If you don\\'t have the kind of sense of humor this film requires, that\\'s one thing. But don\\'t give it a bad review because you think it\\'s \"looney\" but isn\\'t intended to be. Loony is an apt description of both what it intends to be and what it succeeds in being. I laughed much more often and much more intensely at this film than I do at most other funny films.<br /><br />Michael Almereyda brought out the best in his cast in this, his first big film, for which he was nominated for an Independent Spirit award for Best First Feature. Crispin Glover, as usual, is absolutely brilliant in some perfectly inexplicable way, and some of his very funniest lines ever were said in this film. Glover is clearly the best comic actor working right now. Harry Dean Stanton puts in an excellent comic performance. William Burroughs is incredible in a small role. Dylan McDermott was ideally cast, and Lois Chiles is completely excellent.<br /><br />All of the above are perfectly funny in their own perfect way, but this is one of Crispin\\'s finest performances--up there with his performances in River\\'s Edge and Willard. If you are a Crispin Glover fan, but have somehow overlooked this film, you need to stop whatever it is you\\'re doing and see this immediately.<br /><br />But just remember to relax first, and maybe have a beer or something. This is a great film, but it is very unusual, and I recommend being in or inducing an appropriately silly state of mind before viewing.'\n",
      "18\n",
      "b'Let me state this right from the start. I do NOT hate this show. I actually quite like some aspects of it. In fact, when i first started to watch it, I quickly became hooked. I was just starting to come out of the whole \"anime is for kids\" stereotype, and the mature elements of the show had me intrigued.<br /><br />Unfortunately, after seeing the whole series through and a few of the films, I can say that my overall disposition has changed, and it falls into almost all of the pitfalls that plague \"bad\" anime. Seven or eight friends and myself started watching this series on TV. By the end, only one friend and I were still watching and neither of us liked it.<br /><br />Allow me to explain the plot for you. You can skip this paragraph if you don\\'t want to know. Kagome is an average high school student, who one day falls into a magical well near her family run shrine. When kagome comes out of the well again, she has been transported back in time to the feudal era of japan. She meets up with many other characters and they form a group of five or so companions who set off on a journey of revenge/justice/groping in one characters case =). Overall, they are trying to recover the pieces of the sacred jewel shard which enhances the power of demons who use it.<br /><br />While there are many, MANY side stories and story arcs, there is no were near enough material to occupy 167 episodes. The only story arc that is interesting enough to watch is still sort of dull (the band of seven). After the half way mark in the series or maybe even before, it becomes painfully obvious that the plot is frozen in place and whoever made the series decided instead to put in dozens and dozens of filler episodes. <br /><br />These episodes have little to no impact on the story, and rarely even on the characters. In some cases, some characters who had an important role in the story will disappear for dozens of episodes at a time. Many episodes follow the exact same cookie cutter patterns as the stories before it. Inuyasha shoots wind-scar at enemy. Windscar deflects. Characters gasp in horror. Enemy turns out to have barrier. Characters spent three episodes trying to kill enemy before Kagome finally fires sacred arrow at him and he turns to dust. <br /><br />Also **MAJOR SPOILER: THE CONCLUSION WILL BE REVEALED** the lack of any conclusion makes it seem like you have waisted 83 hours. <br /><br />**MAJOR SPOILER OVER**<br /><br />The animation itself is above average, and in some cases excellent. Even so, reused animation cells plague most action scenes, and it is very hard to ignore them when it is clear that the exact same boulder has flown past a character five or six times in a row.<br /><br />On the brighter side however, all of the characters are very well developed and the romances between some of the characters were truly captivating. Also, the character designs (appearences) were brilliant and at times among the best I have seen, particularly with the band of seven. There is definitely no shortage of Cosplay opportunities here. Even so, I found myself hoping that a character would die just so there would be some sort of movement in the plot. And some of the humour in the show between characters is used again and again. One particular joke (sit boy) is found within the first five episodes, and you can literally expect it to be used again and again for the remaining 162 episodes.<br /><br />Although there are some good aspects of the show and it is easy to see why it has a huge following, the series seems to be dominated by obsessed fan girls who drool over Sesshomiru and InuYasha.<br /><br />Bottom line: Definitely worth checking out, but not worth watching the whole series. The first 30 episodes are very clever, original and enjoyable for anybody. But after that, it simply becomes dull and tedious. Watching a TV show should never feel like a chore, but somehow this series accomplishes just that. Don\\'t expect much from \"InuYasha\", because you will only feel let down.'\n",
      "19\n",
      "b\"This film is great. All the hi-tech machinery and technology is mind-boggling. It is packed with action, humour and not to mention, guys. You will want to see it again and again. Very very funny. Also, it has a very unique plot which is unpredictable. You wouldn't want to miss out on it.\"\n",
      "20\n",
      "b\"The Bone Collector is set in New York City & starts as one of the world's foremost criminologist's & crime scene experts Lincoln Rhyme (Denzel Washington) is involved in an accident which leaves him a bedridden quadriplegic. Jump forward four years & Alan (Gary Swanson) & his wife Lindsay Rubin (Olivia Birkelund) are kidnapped, soon after New York cop Amelia Donaghy (Angelina Jolie) is called to a crime scene & finds the buried & mutilated body of Alan. Amelia notices some unusual crime scene evidence & makes a note of it which impresses Rhyme when he is asked to work on the case, he quickly realises the evidence are in fact cryptic clues to the whereabouts of Lindsay. Having cracked the clues the cops get there too late to save her but this is just the beginning as a sadistic serial killer continues to kill & leave forensic clues for Rhyme & the police...<br /><br />Directed by Phillip Noyce I watched The Bone Collector last night & I have to say it's one of the worst big budget post The Silence of the Lambs (1991) & Se7en (1995) serial killer thrillers I have seen, in fact it makes Friday the 13th (1980) look sophisticated & realistic! The script by Jeremy Iacone was based on the book by Jeffery Deaver & is so poor on so many levels I hardly know where to begin. For a start it takes itself deadly seriously & that makes all the other flaws seem twice as bad. The character's are truly awful & I didn't believe any of them were actual human beings. First we have Lincoln Rhyme who is paralysed from the neck down & there's just not a lot the script can do with him, in fact he quite literally can't do anything but lie in bed for the whole film. He is seemingly impressed with Amelia because she stopped a train & thought a fresh footprint near a murdered person might be of relevance, I'm not being funny here but wouldn't any cop realise a footprint near a murder victim might be of some relevance? Why is he so impressed with her? Then there's Capatin Cheney who is not only unlikable & shouts at everyone for no apparent reason but is so incompetent that he failed to connect several murders committed in a short space of time where each victim had sections of flesh & skin surgically removed from their bodies, how exactly did this guy get to be a police Captain? Then there's the killer whose motives are less than plausible, are you trying to tell me they devised an intricate plan to murder at least seven people because they spent six years in jail for something they actually did? If they wanted revenge on Rhyme why did they kill all those other people who had no connection to anything, I could maybe just about buy someone wanting revenge against the guy who put them away but not to kill several other people who have no connection to themselves, the intended target Rhyme anything else. Also after devising an intricate plan to kill these people & get away with it they suddenly turn into the most stupid person in history as despite holding a large knife & being able to walk & use their arms they are actually defeated & nearly killed by a quadriplegic who has no movement in his body below his neck! How did that happen? I should also mention Amelia who is a terrible character, she actually buys her own camera to take crime scene photo's & shoots rats for no apparent reason.<br /><br />Besides some of the worst written character's ever the story & plot isn't much better We never find out why the killer is using The Bone Collector book as inspiration We never find out why the killer was taking strips of flesh from his victims. It's never explained why a rookie cop like Amelia is allowed to enter crime scenes even before the proper forensic teams. There is no reason given for why the killer chooses his victims. Also the killers clues are a little obscure aren't they? I mean a bloody animal bone & shaved rat hair? Logically how does someone go from a bone & rat hair to the exact pinpoint location of the next victim & has the whole of New York to choose from? There's some nonsense about a bird that sits on Rhymes window ledge which is just totally random & at almost two hours The Bone Collector is really slow going. There is so much wrong with The Bone Collector & it all comes down to one of the worst scripts ever, it's atrocious on all levels & has zero credibility. Apparently Angelina Jolie has stated that she shot nude scenes for this film but they were cut because they were felt to be too distracting.<br /><br />With a supposed budget of about $48,000,000 The Bone Collector is well made with good production values & that Hollywood gloss about it. I also must add right now that I think Angelina Jolie gives one of the worst performances I have ever seen, I think she is absolutely terrible in this. Denzel Washington just sort of lies there really, Queen Latifah is awful & even Michael Rooker can't do much as he is stuck with a clich\\xc3\\xa9d & one dimensional character.<br /><br />The Bone Collector has to be one of the worst Hollywood films I have seen in a while, I saw it for free on telly last night & I still feel cheated & ripped-off. There are just so many things to poke holes at it's silly, embarrassingly awful or should that be awfully embarrassing? Works either way to be honest...\"\n",
      "21\n",
      "b'\"Dead Man Walking\" is a film not about the death penalty, but about the people involved in a death penalty case -- the killer, the families whose kids were killed, the nun who becomes his spiritual advisor, and what happens. It tells the story with little fanfare but a lot of compassion and sensitivity. I have it on DVD, and every time I watch it (not often, it\\'s never an easy film to watch) I\\'m more impressed by what Tim Robbins and the entire cast did here. So revealing that it could be a documentary, so compelling you can\\'t take your eyes away, so subtle and yet so powerful... \"Dead Man Walking\" is nothing short of a masterpiece. It doesn\\'t matter whether you\\'re for or against the death penalty (or even have no opinion), this movie will have you thinking about the issues for sure. It takes a courageous screenwriter and director to look this material in the face without flinching even once, and everyone involved in the film pulls it off -- there isn\\'t a single scene that rings false. A masterful film, but don\\'t expect light viewing... to some, the final scenes could be more graphic than anything imaginable, even though no blood or violence is shown. You get drawn into this film and become a participant, and there\\'s a character for just about everyone to identify with. 10/10.'\n",
      "22\n",
      "b'Sitting in a big wing chair with a huge book in his lap, the one and only Bela Lugosi looks into the camera and, in a dreadful vocal delivery that sounds as if he were mocking a reading of Shakespeare, intones sloooow-ly: \"Man\\'s constant groping of things unknown, drawing from the endless reaches of time, brings to light many startling things; (snicker); startling?, because they seem new (Lugosi\\'s eyes now bulging, with raised eyebrows, and mouth sneering, he continues) but most are not new, the signs of the ages\" (cue a visual of lightening, accompanied by the sound of thunder which then continues to rumble for an astonishing 86 seconds).<br /><br />And so begins what is arguably the worst film ever made. This \"movie\" almost defies description. Told in semi-docudrama style with an unseen narrator explaining the plot ... such as it is ... the story revolves around the vicissitudes of a man named Glen (Ed Wood, Jr.) who cross-dresses; hence the reference to Glenda. The film has no real structure. Instead, it consists mostly of a random assortment of vignettes that may ... or may not ... relate to Glen or to the cross-dressing motif. One long sequence consists of some unknown woman wriggling on a sofa, followed by a man whipping a woman in what we would today refer to as S&M.<br /><br />Then, at odd moments Bela reappears, for no apparent reason, and babbles more inane dialogue, like: \"When he\\'s wrong because he does right, and when he\\'s right because he does wrong; pull the string, dance to that.\" Huh?<br /><br />About twenty percent of the film\\'s visuals consist of stock footage, accompanied by a VO that relates to the story motif but not the visuals. Hence, we see stock footage of: bustling city streets, freeway traffic, a thunderous herd of buffalo, and a playground full of kids. But it gets worse. In a film about cross-dressing, we have 58 consecutive seconds of stock footage of a foundry furnace making hot steel, and 84 consecutive seconds of battle scenes from WWII.<br /><br />Even the simplest items are botched. In one scene we see a newspaper headline that reads \"Man Nabbed Dressed as Girl\". Underneath the headline, which has clearly been glued or pasted on, the article is about ... taxes. In one of my favorite scenes, an off-screen woman spouts out: \"airplanes, why it\\'s against the creator\\'s will\", in a voice that sounds like she\\'s just inhaled helium.<br /><br />Except for the performance of Lyle Talbot, the acting is uniformly horrendous. Production design is cheap looking and drab; (but you gotta love that tacky wallpaper). The editing is sloppy. Most of the background music is suitable only for 1950 style elevators. The B&W cinematography has way too much contrast. And the costumes look like something that came from a thrift store.<br /><br />This film is so bad it makes \"Plan 9 From Outer Space\" look like \"Citizen Kane\", by comparison. I just don\\'t know how one could make a film any worse than Ed Wood\\'s \"Glen Or Glenda\". But thankfully, it\\'s got Bela Lugosi in it. Every time he opened his mouth, and gazed into the camera with those big, bulging eyes, I about fell on the floor laughing.'\n",
      "23\n",
      "b\"This film makes about as much sense as an 'Ozzie & Harriet' or a 'Father Knows Best' episode. An old copy of Reader's Digest (circa 1962) would provide more insight into modern life, or the relationship between a father and a daughter, than this weird concoction.<br /><br />I was surprised with Diane Keaton. She appears to sleepwalk through the film. (Given the film's title, I realize that hers was a supporting role but even Martin Short managed a distinct, supporting character.)<br /><br />I can understand the attraction of an imaginary world created in a good romantic comedy. But this film is the prozac version of an imaginary world. I'm frightened to consider that anyone could enjoy it even as pure fantasy.\"\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for it in test_data_2:\n",
    "    if cnt % 1000 == 0:\n",
    "        print(str(it))\n",
    "        print(cnt //1000)\n",
    "    cnt += 1\n",
    "    tokenizer.fit_on_texts(str(it))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "tokenized_word_list_test = []\n",
    "for it in test_data_2:\n",
    "    if cnt % 1000 == 0:\n",
    "        print(len(tokenized_word_list_test))\n",
    "    cnt += 1\n",
    "    tokenized_word_list_test.append(tokenizer.texts_to_sequences(str(it)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_word_list_2_test = []\n",
    "for it in test_data_2:\n",
    "    temp = tokenizer.texts_to_sequences(str(it))\n",
    "    newL = []\n",
    "    for it2 in temp:\n",
    "        if it2 == []:\n",
    "            continue\n",
    "        newL.append(it2[0])\n",
    "    tokenized_word_list_2_test.append(newL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_padded = pad_sequences(tokenized_word_list_2_test, maxlen = maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = tf.keras.models.load_model('homework2/imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 29s 38ms/step - loss: 0.5222 - accuracy: 0.7711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5221703052520752, 0.771120011806488]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.evaluate(np.array(X_test_padded), np.array(test_label_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_padded, test_label_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 128\n",
    "MAX_TOKENS = 5000\n",
    "\n",
    "# load the text dataset\n",
    "\n",
    "\n",
    "# Create TextVectorization layer\n",
    "vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQ_LEN)\n",
    "\n",
    "# Use `adapt` to create a vocabulary mapping words to integers\n",
    "#train_text = \n",
    "vectorize_layer.adapt(train_data.map(lambda x: x['text']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print out a batch to see what it looks like in text and in integers\n",
    "for text in train_text:\n",
    "    text = tf.convert_to_tensor([text], dtype='string')\n",
    "    print(list(zip(text.numpy(), vectorize_layer(text).numpy())))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vectorize_layer.get_vocabulary())\n",
    "EMBEDDING_SIZE = int(np.sqrt(VOCAB_SIZE))\n",
    "print(\"Vocab size is {} and is embedded into {} dimensions\".format(VOCAB_SIZE, EMBEDDING_SIZE))\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in validation_data:\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_input = Input(shape=(None,), dtype='float')\n",
    "value_input = Input(shape=(None,), dtype='float')\n",
    "\n",
    "# Query embeddings of shape [batch_size, Tq, dimension].\n",
    "query_embeddings = embedding_layer(query_input)\n",
    "# Value embeddings of shape [batch_size, Tv, dimension].\n",
    "value_embeddings = embedding_layer(value_input)\n",
    "\n",
    "# CNN layer.\n",
    "cnn_layer = Conv1D(\n",
    "    filters=100,\n",
    "    kernel_size=4,\n",
    "    padding='same')\n",
    "# Query encoding of shape [batch_size, Tq, filters].\n",
    "query_seq_encoding = cnn_layer(query_embeddings)\n",
    "# Value encoding of shape [batch_size, Tv, filters].\n",
    "value_seq_encoding = cnn_layer(value_embeddings)\n",
    "\n",
    "# Query-value attention of shape [batch_size, Tq, filters].\n",
    "query_value_attention_seq = tf.keras.layers.Attention()(\n",
    "    [query_seq_encoding, value_seq_encoding])\n",
    "\n",
    "# Reduce over the sequence axis to produce encodings of shape\n",
    "# [batch_size, filters].\n",
    "query_encoding = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "    query_seq_encoding)\n",
    "query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "    query_value_attention_seq)\n",
    "print(query_value_attention)\n",
    "# Concatenate query and document encodings to produce a DNN input layer.\n",
    "input_layer = tf.keras.layers.Concatenate()([query_encoding, query_value_attention])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_data_2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIDI1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1024, return_sequences=True))\n",
    "GlobalPool = tf.keras.layers.GlobalMaxPooling1D()\n",
    "LSTM1 = tf.keras.layers.LSTM(512, return_sequences = True)\n",
    "LSTM2 = tf.keras.layers.LSTM(256, return_sequences = True)\n",
    "gru1 = tf.keras.layers.GRU(256, return_sequences = True) \n",
    "gru2 = tf.keras.layers.GRU(256)\n",
    "\n",
    "# input_layer = BIDI1(input_layer)\n",
    "# input_layer = GlobalPool(input_layer)\n",
    "# input_layer = tf.keras.layers.LSTM(512)(input_layer)\n",
    "# input_layer = tf.keras.layers.LSTM(256)(input_layer)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "model = keras.Model(inputs=input_layer, outputs=output_layer, name=\"crying_model\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll make a conv layer to produce the query and value tensors\n",
    "query_layer = tf.keras.layers.Conv1D(\n",
    "    filters=100,\n",
    "    kernel_size=4,\n",
    "    padding='same')\n",
    "value_layer = tf.keras.layers.Conv1D(\n",
    "    filters=100,\n",
    "    kernel_size=4,\n",
    "    padding='same')\n",
    "# Then they will be input to the Attention layer\n",
    "attention = tf.keras.layers.Attention()\n",
    "concat = tf.keras.layers.Concatenate()\n",
    "\n",
    "cells = [tf.keras.layers.LSTMCell(256), tf.keras.layers.LSTMCell(64)]\n",
    "rnn = tf.keras.layers.RNN(cells)\n",
    "output_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "cnt = 0\n",
    "loss_values = []\n",
    "for epoch in range(5):\n",
    "    loss_values_per_epoch = []\n",
    "    for batch in train_data.batch(32):\n",
    "        text = batch['text']\n",
    "        embeddings = embedding_layer(vectorize_layer(text))\n",
    "        query = query_layer(embeddings)\n",
    "        value = value_layer(embeddings)\n",
    "        query_value_attention = attention([query, value])\n",
    "        #print(\"Shape after attention is (batch, seq, filters):\", query_value_attention.shape)\n",
    "        attended_values = concat([query, query_value_attention])\n",
    "        #print(\"Shape after concatenating is (batch, seq, filters):\", attended_values.shape)\n",
    "        logits = output_layer(rnn(attended_values))\n",
    "        loss = tf.keras.losses.binary_crossentropy(tf.expand_dims(batch['label'], -1), logits, from_logits=True)\n",
    "        loss_values_per_epoch.append(loss)\n",
    "    \n",
    "print(tf.reduce_mean(loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-env)",
   "language": "python",
   "name": "tensorflow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
