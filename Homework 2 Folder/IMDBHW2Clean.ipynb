{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-u3z_kt63 because the default path (/home/cse479/izzatadly/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from __future__ import print_function\n",
    "import numpy as np                 # to use numpy arrays\n",
    "import tensorflow as tf            # to specify and run computation graphs\n",
    "import tensorflow_datasets as tfds # to load training data\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import Model, initializers, regularizers, constraints\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Embedding, GlobalMaxPooling1D, Dense, Flatten, Conv2D, MaxPooling2D, MaxPool2D, Dropout, GlobalAvgPool2D, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "import math as m\n",
    "import matplotlib.pyplot as plt    # to visualize data and draw plots\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = tfds.load(\n",
    "    name=\"imdb_reviews\", \n",
    "    split=('train', 'test'),\n",
    "    as_supervised=False)\n",
    "\n",
    "temp = train_data.as_numpy_iterator()\n",
    "train_data_2 = []\n",
    "train_label_2 = []\n",
    "for it in temp:\n",
    "    train_data_2.append(it['text'])\n",
    "    train_label_2.append(it['label'])\n",
    "    \n",
    "temp = test_data.as_numpy_iterator()\n",
    "test_data_2 = []\n",
    "test_label_2 = []\n",
    "for it in temp:\n",
    "    test_data_2.append(it['text'])\n",
    "    test_label_2.append(it['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training data\n",
    "vocab_size = 60000\n",
    "maxlen = 250\n",
    "encode_dim = 70\n",
    "batch_size = 32\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Tokenize training data\n",
    "for it in train_data_2:\n",
    "    tokenizer.fit_on_texts(str(it))\n",
    "    \n",
    "tokenized_word_list_train_2 = []\n",
    "for it in train_data_2:\n",
    "    temp = tokenizer.texts_to_sequences(str(it))\n",
    "    newL = []\n",
    "    for it2 in temp:\n",
    "        if it2 == []:\n",
    "            continue\n",
    "        newL.append(it2[0])\n",
    "    tokenized_word_list_train_2.append(newL)\n",
    "    \n",
    "X_train_padded = pad_sequences(tokenized_word_list_train_2, maxlen = maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the test data\n",
    "\n",
    "for it in train_data_2:\n",
    "    tokenizer.fit_on_texts(str(it))\n",
    "\n",
    "tokenized_word_list_test_2 = []\n",
    "for it in train_data_2:\n",
    "    temp = tokenizer.texts_to_sequences(str(it))\n",
    "    newL = []\n",
    "    for it2 in temp:\n",
    "        if it2 == []:\n",
    "            continue\n",
    "        newL.append(it2[0])\n",
    "    tokenized_word_list_test_2.append(newL)\n",
    "    \n",
    "X_test_padded = pad_sequences(tokenized_word_list_test_2, maxlen = maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention layer\n",
    "class AttentionWithContext(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, W_regularizer=None, u_regularizer=None, b_regularizer=None,                \n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape = (input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name = 'name',\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape = (input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name = 'name',\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight(shape = (input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name = 'name',\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "        \n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = tf.math.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = tf.math.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= tf.cast(mask, tf.float32)\n",
    "        \n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= tf.cast(tf.math.reduce_sum(a, axis=1, keepdims=True) + backend.epsilon(), tf.float32)\n",
    "\n",
    "        a = tf.expand_dims(a, -1)\n",
    "        print(x.shape)\n",
    "        print(a.shape)\n",
    "        weighted_input = x * a\n",
    "        return tf.math.reduce_sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    return tf.tensordot(x, kernel, axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embed = Embedding(input_dim = vocab_size, output_dim = 20, input_length = X_train_padded.shape[1])\n",
    "\n",
    "model.add(embed)\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Bidirectional(GRU(256, return_sequences = True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(GRU(256, return_sequences = True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(GRU(256, return_sequences = True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(AttentionWithContext())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(200))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inputs are actually X_train_padded, and the label is train_label_2\n",
    "X_train_final2, X_val, y_train_final2, y_val = train_test_split(X_train_padded, train_label_2, test_size = 0.2, shuffle=True)\n",
    "\n",
    "X_train_final2 = np.array(X_train_final2)\n",
    "X_val = np.array(X_val)\n",
    "y_train_final2 = np.array(y_train_final2)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Fitting the model\n",
    "history = model.fit(X_train_final2, y_train_final2, epochs = 20 , batch_size = batch_size, verbose = 1, validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final2 = X_test_padded\n",
    "Y_test_final2 = test_label_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(validation_labels, y_pred):\n",
    "    con_Matrix = tf.math.confusion_matrix(labels=validation_labels, predictions=y_pred).numpy()\n",
    "\n",
    "    con_Matrix_norm = np.around(con_Matrix.astype('float') / con_Matrix.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "    con_mat_df = pd.DataFrame(con_Matrix_norm, index = classes, columns = classes)\n",
    "\n",
    "\n",
    "    #plots the heat map of the con matrix\n",
    "    figure = plt.figure()\n",
    "    sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "def confidence_interval(validation_evaluation, validation_size):\n",
    "    validation_data_error = 1 - validation_evaluation[1]\n",
    "    print(validation_data_error)\n",
    "\n",
    "    lower_bound_interval = validation_data_error - 1.96 * sqrt(\n",
    "        (validation_data_error * (1 - validation_data_error)) / validation_size)\n",
    "    upper_bound_interval = validation_data_error + 1.96 * sqrt(\n",
    "        (validation_data_error * (1 - validation_data_error)) / validation_size)\n",
    "    return (lower_bound_interval, upper_bound_interval)\n",
    "\n",
    "def print_accuracy_graph(history):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def print_loss_graph(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 24s 150ms/step - loss: 0.5900 - accuracy: 0.6638\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-86e733676a41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_final2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_final2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#for confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test_final2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint_accuracy_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint_loss_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-80780abf4858>\u001b[0m in \u001b[0;36mprint_confusion_matrix\u001b[0;34m(validation_labels, y_pred)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcon_Matrix_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon_Matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcon_Matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcon_mat_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon_Matrix_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "validation_evaluation = model.evaluate(X_test_final2, Y_test_final2) # Stores [loss, accuracy] of model\n",
    "\n",
    "#all until functions\n",
    "y_predict = model.predict(X_test_final2)\n",
    "y_pred = model.predict_classes(X_test_final2) #for confusion matrix\n",
    "print_confusion_matrix(Y_test_final2, y_pred)\n",
    "print_accuracy_graph(history)\n",
    "print_loss_graph(history)\n",
    "confidence_interval(validation_evaluation, len(Y_test_final2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_FMNIST.print_confusion_matrix(X_test_final2,y_pred)\n",
    "util_FMNIST.print_accuracy_graph(history)\n",
    "util_FMNIST.print_loss_graph(history)\n",
    "lower_bound_interval, upper_bound_interval = util_FMNIST.confidence_interval(validation_evaluation, len(Y_test_final2))\n",
    "print(\"The 95% Confidence interval for error hypothesis based on the normal distribution estimator: \",\n",
    "      (lower_bound_interval, upper_bound_interval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(model, 'homework1/FMNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-env)",
   "language": "python",
   "name": "tensorflow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
