{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from __future__ import print_function\n",
    "import numpy as np                 # to use numpy arrays\n",
    "import tensorflow as tf            # to specify and run computation graphs\n",
    "import tensorflow_datasets as tfds # to load training data\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras import backend\n",
    "from tensorflow.keras import Model, initializers, regularizers, constraints\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Reshape, Bidirectional, Dense, Input, Dropout, LeakyReLU, Concatenate, PReLU, Flatten\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = pd.read_csv('input.csv')\n",
    "\n",
    "keys = []\n",
    "for it in inputs:\n",
    "    keys.append(it)\n",
    "    \n",
    "distribution = [4, 9, 24, 26, 27, 28] #horrible design\n",
    "\n",
    "number_of_tests = len(inputs[keys[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_short_term = []\n",
    "input_long_term = []\n",
    "output = []\n",
    "\n",
    "for i in range(number_of_tests):\n",
    "    instance = []\n",
    "    if pd.isna(inputs[keys[0]][i]):\n",
    "        continue\n",
    "    for j in range(distribution[0], distribution[1]):         \n",
    "        instance.append(float(inputs[keys[j]][i])/100)\n",
    "\n",
    "    input_short_term.append(instance)\n",
    "    instance = []\n",
    "    for j in range(distribution[1], distribution[2]):\n",
    "        instance.append(float(inputs[keys[j]][i])/100)\n",
    "    for j in range(distribution[3], distribution[4]):\n",
    "        instance.append(float(inputs[keys[j]][i])/100)\n",
    "    input_long_term.append(instance)\n",
    "\n",
    "    output.append(float(inputs[keys[distribution[5] - 1]][i])/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    }
   ],
   "source": [
    "print(len(input_short_term))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending both short term and long term variables into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_short_term = []\n",
    "training_input_long_term = []\n",
    "training_output = []\n",
    "\n",
    "testing_input_short_term = []\n",
    "testing_input_long_term = []\n",
    "testing_output = []\n",
    "\n",
    "for i in range(8):\n",
    "    for j in range(11):\n",
    "        training_input_short_term.append(input_short_term[j])\n",
    "        training_input_long_term.append(input_long_term[j])\n",
    "        training_output.append(output[j])\n",
    "        \n",
    "        \n",
    "    del input_short_term[0:11]\n",
    "    del input_long_term[0:11]\n",
    "    del output[0:11]\n",
    "        #training_input_short_term = np.delete(input_short_term, j)\n",
    "        \n",
    "    for k in range(3):\n",
    "        testing_input_short_term.append(input_short_term[k])\n",
    "        testing_input_long_term.append(input_long_term[k])\n",
    "        testing_output.append(output[k])\n",
    "        \n",
    "    del input_short_term[0:3]\n",
    "    del input_long_term[0:3]\n",
    "    del output[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "print(len(training_input_short_term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification that training and testing data set are correct length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n",
      "88\n",
      "88\n",
      "24\n",
      "24\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# Verification \n",
    "print(len(training_input_short_term))\n",
    "print(len(training_input_long_term))\n",
    "print(len(training_input_long_term))\n",
    "\n",
    "print(len(testing_input_short_term))\n",
    "print(len(testing_input_long_term))\n",
    "print(len(testing_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating each specific long term variables training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "Capital_Investment_train = []\n",
    "Labor_Force_Participation_train = []\n",
    "Fixed_Broadband_train = []\n",
    "RandD_train = []\n",
    "Property_Rights_train = []\n",
    "Freedom_From_Corruption_train = []\n",
    "Fiscal_Freedom_train = []\n",
    "Business_Freedom_train = []\n",
    "Labor_Freedom_train = []\n",
    "Monetary_Freedom_train = []\n",
    "Trade_Freedom_train = []\n",
    "Investment_Freedom_train = []\n",
    "Financial_Freedom_train = []\n",
    "Economic_Freedom_Overall_train = []\n",
    "Pop_Above_65_train = []\n",
    "Savings_As_GDP_train = []\n",
    "\n",
    "\n",
    "for i in range(len(training_input_long_term)):\n",
    "    Capital_Investment_train.append(training_input_long_term[i][0])\n",
    "    Labor_Force_Participation_train.append(training_input_long_term[i][1])\n",
    "    Fixed_Broadband_train.append(training_input_long_term[i][2])\n",
    "    RandD_train.append(training_input_long_term[i][3])\n",
    "    Property_Rights_train.append(training_input_long_term[i][4])\n",
    "    Freedom_From_Corruption_train.append(training_input_long_term[i][5])\n",
    "    Fiscal_Freedom_train.append(training_input_long_term[i][6])\n",
    "    Business_Freedom_train.append(training_input_long_term[i][7])\n",
    "    Labor_Freedom_train.append(training_input_long_term[i][8])\n",
    "    Monetary_Freedom_train.append(training_input_long_term[i][9])\n",
    "    Trade_Freedom_train.append(training_input_long_term[i][10])\n",
    "    Investment_Freedom_train.append(training_input_long_term[i][11])\n",
    "    Financial_Freedom_train.append(training_input_long_term[i][12])\n",
    "    Economic_Freedom_Overall_train.append(training_input_long_term[i][13])\n",
    "    Pop_Above_65_train.append(training_input_long_term[i][14])\n",
    "    Savings_As_GDP_train.append(training_input_long_term[i][15])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating each specific long term variable test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "Capital_Investment_test = []\n",
    "Labor_Force_Participation_test  = []\n",
    "Fixed_Broadband_test  = []\n",
    "RandD_test  = []\n",
    "Property_Rights_test  = []\n",
    "Freedom_From_Corruption_test  = []\n",
    "Fiscal_Freedom_test  = []\n",
    "Business_Freedom_test  = []\n",
    "Labor_Freedom_test  = []\n",
    "Monetary_Freedom_test  = []\n",
    "Trade_Freedom_test  = []\n",
    "Investment_Freedom_test  = []\n",
    "Financial_Freedom_test  = []\n",
    "Economic_Freedom_Overall_test  = []\n",
    "Pop_Above_65_test  = []\n",
    "Savings_As_GDP_test  = []\n",
    "\n",
    "    \n",
    "for i in range(len(testing_input_long_term)):\n",
    "    Capital_Investment_test.append(testing_input_long_term[i][0])\n",
    "    Labor_Force_Participation_test.append(testing_input_long_term[i][1])\n",
    "    Fixed_Broadband_test.append(testing_input_long_term[i][2])\n",
    "    RandD_test.append(testing_input_long_term[i][3])\n",
    "    Property_Rights_test.append(testing_input_long_term[i][4])\n",
    "    Freedom_From_Corruption_test.append(testing_input_long_term[i][5])\n",
    "    Fiscal_Freedom_test.append(testing_input_long_term[i][6])\n",
    "    Business_Freedom_test.append(testing_input_long_term[i][7])\n",
    "    Labor_Freedom_test.append(testing_input_long_term[i][8])\n",
    "    Monetary_Freedom_test.append(testing_input_long_term[i][9])\n",
    "    Trade_Freedom_test.append(testing_input_long_term[i][10])\n",
    "    Investment_Freedom_test.append(testing_input_long_term[i][11])\n",
    "    Financial_Freedom_test.append(testing_input_long_term[i][12])\n",
    "    Economic_Freedom_Overall_test.append(testing_input_long_term[i][13])\n",
    "    Pop_Above_65_test.append(testing_input_long_term[i][14])\n",
    "    Savings_As_GDP_test.append(testing_input_long_term[i][15])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall training and testing arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_short_term = np.array(training_input_short_term)\n",
    "training_input_long_term = np.array(training_input_long_term)\n",
    "training_output = np.array(training_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_input_short_term = np.array(testing_input_short_term)\n",
    "testing_input_long_term = np.array(testing_input_long_term)\n",
    "testing_output = np.array(testing_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Each specific array of long term variables training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "Capital_Investment_train = np.array(Capital_Investment_train)\n",
    "Capital_Investment_test = np.array(Capital_Investment_test)\n",
    "\n",
    "Labor_Force_Participation_train = np.array(Labor_Force_Participation_train)\n",
    "Labor_Force_Participation_test  = np.array(Labor_Force_Participation_test)\n",
    "\n",
    "Fixed_Broadband_train = np.array(Fixed_Broadband_train)\n",
    "Fixed_Broadband_test  = np.array(Fixed_Broadband_test)\n",
    "\n",
    "RandD_train = np.array(RandD_train)\n",
    "RandD_test  = np.array(RandD_test)\n",
    "\n",
    "Property_Rights_train = np.array(Property_Rights_train)\n",
    "Property_Rights_test = np.array(Property_Rights_test)\n",
    "\n",
    "Freedom_From_Corruption_train = np.array(Freedom_From_Corruption_train)\n",
    "Freedom_From_Corruption_test  = np.array(Freedom_From_Corruption_test)\n",
    "\n",
    "Fiscal_Freedom_train = np.array(Fiscal_Freedom_train)\n",
    "Fiscal_Freedom_test  = np.array(Fiscal_Freedom_test)\n",
    "\n",
    "Business_Freedom_train = np.array(Business_Freedom_train)\n",
    "Business_Freedom_test  = np.array(Business_Freedom_test)\n",
    "\n",
    "Labor_Freedom_train = np.array(Labor_Freedom_train)\n",
    "Labor_Freedom_test  = np.array(Labor_Freedom_test)\n",
    "\n",
    "Monetary_Freedom_train = np.array(Monetary_Freedom_train)\n",
    "Monetary_Freedom_test  = np.array(Monetary_Freedom_test)\n",
    "\n",
    "Trade_Freedom_train = np.array(Trade_Freedom_train)\n",
    "Trade_Freedom_test  = np.array(Trade_Freedom_test)\n",
    "\n",
    "Investment_Freedom_train = np.array(Investment_Freedom_train)\n",
    "Investment_Freedom_test  = np.array(Investment_Freedom_test)\n",
    "\n",
    "Financial_Freedom_train = np.array(Financial_Freedom_train)\n",
    "Financial_Freedom_test  = np.array(Financial_Freedom_test)\n",
    "\n",
    "Economic_Freedom_Overall_train = np.array(Economic_Freedom_Overall_train)\n",
    "Economic_Freedom_Overall_test  = np.array(Economic_Freedom_Overall_test)\n",
    "\n",
    "Pop_Above_65_train = np.array(Pop_Above_65_train)\n",
    "Pop_Above_65_test  = np.array(Pop_Above_65_test)\n",
    "\n",
    "Savings_As_GDP_train = np.array(Savings_As_GDP_train)\n",
    "Savings_As_GDP_test  = np.array(Savings_As_GDP_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "print(len(Capital_Investment_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the long term to be put into a specific model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to do each country with each long term variable manually to get the randomized complete block design\n",
    "##  Train  Test\n",
    "# [0:11] [0:3] - Canada\n",
    "# [11:22] [3:6] - Greece\n",
    "# [22:33] [6:9] - Japan\n",
    "# [33:44] [9:12] - Malaysia\n",
    "# [44:55] [12:15] - South Africa\n",
    "# [55:66] [15:18] - Spain\n",
    "# [66:77] [18:21] - UK\n",
    "# [77:88] [21:24] - USA\n",
    "# We need to do each country with each long term variable manually to get the randomized complete block design\n",
    "## #  Train  Test\n",
    "# [0:11] [0:3] - Canada\n",
    "# [11:22] [3:6] - Greece\n",
    "# [22:33] [6:9] - Japan\n",
    "# [33:44] [9:12] - Malaysia\n",
    "# [44:55] [12:15] - South Africa\n",
    "# [55:66] [15:18] - Spain\n",
    "# [66:77] [18:21] - UK\n",
    "# [77:88] [21:24] - USA\n",
    "\n",
    "trainLV = [Capital_Investment_train, Labor_Force_Participation_train, Fixed_Broadband_train,\n",
    "          RandD_train, Property_Rights_train, Freedom_From_Corruption_train, Fiscal_Freedom_train, Business_Freedom_train,\n",
    "          Labor_Freedom_train, Monetary_Freedom_train, Trade_Freedom_train, Investment_Freedom_train, Financial_Freedom_train,\n",
    "          Economic_Freedom_Overall_train, Pop_Above_65_train, Savings_As_GDP_train]\n",
    "\n",
    "testLV = [Capital_Investment_test, Labor_Force_Participation_test, Fixed_Broadband_test,\n",
    "          RandD_test, Property_Rights_test, Freedom_From_Corruption_test, Fiscal_Freedom_test, Business_Freedom_test,\n",
    "          Labor_Freedom_train, Monetary_Freedom_test, Trade_Freedom_test, Investment_Freedom_test, Financial_Freedom_test,\n",
    "          Economic_Freedom_Overall_test, Pop_Above_65_train, Savings_As_GDP_test]\n",
    "\n",
    "# Designated code: Capital Invest = 0\n",
    "#                  Labor Force = 1\n",
    "#                  Fixed Broadband = 2\n",
    "#                  R&D = 3\n",
    "#                  Property rights = 4\n",
    "#                  Free from corruption = 5\n",
    "#                  Fiscal Freedom = 6\n",
    "#                  Business Freedom = 7\n",
    "#                  Labor Freedom = 8\n",
    "#                  Monetary Freedom = 9\n",
    "#                  Trade Freedom = 10\n",
    "#                  Investment Freedom = 11\n",
    "#                  Financial Freedom = 12\n",
    "#                  Economic Freedom = 13\n",
    "#                  Pop above 65 = 14\n",
    "#                  Savings as gdp = 15\n",
    "\n",
    "# Step 1, only the trainLV and testLV designated code will change. \n",
    "# This is because we are the training for the same country but with different long term variables to get the RMSE.\n",
    "# Hence, only the slicing for the long term variable changes. \n",
    "\n",
    "# When we finish all long term variables for one country:\n",
    "# Step 2, change the slicing to a new country and repeat step 1. Reset the designated code back to 0 from 14.\n",
    "\n",
    "training_input_short_term_slice = training_input_short_term[0:11] # 0:11 refers to slicing for country training data\n",
    "training_input_long_term1 = trainLV[3][0:11]   # The 3 is the designated code\n",
    "training_output_slice = training_output[0:11] \n",
    "\n",
    "testing_input_short_term_slice = testing_input_short_term[0:3] # 0:3 refers to slicing for country test data\n",
    "testing_input_long_term1 = testLV[3][0:3] \n",
    "testing_output_slice = training_output[0:3] \n",
    "\n",
    "#training_input_long_term2 = Capital_Investment_train\n",
    "#testing_input_long_term2 = Capital_Investment_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Single long term variable model (Accurate to the project plan from milestone 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_133\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_375 (InputLayer)          [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_609 (Dense)               (None, 50)           300         input_375[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_312 (LeakyReLU)     (None, 50)           0           dense_609[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_697 (Dropout)           (None, 50)           0           leaky_re_lu_312[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_610 (Dense)               (None, 50)           2550        dropout_697[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_313 (LeakyReLU)     (None, 50)           0           dense_610[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_698 (Dropout)           (None, 50)           0           leaky_re_lu_313[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_611 (Dense)               (None, 50)           2550        dropout_698[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_314 (LeakyReLU)     (None, 50)           0           dense_611[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_376 (InputLayer)          [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_377 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_220 (Concatenate)   (None, 56)           0           leaky_re_lu_314[0][0]            \n",
      "                                                                 input_376[0][0]                  \n",
      "                                                                 input_377[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_699 (Dropout)           (None, 56)           0           concatenate_220[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_612 (Dense)               (None, 56)           3192        dropout_699[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_700 (Dropout)           (None, 56)           0           dense_612[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_253 (Reshape)           (None, 1, 56)        0           dropout_700[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_583 (LSTM)                 (None, 1, 100)       62800       reshape_253[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_701 (Dropout)           (None, 1, 100)       0           lstm_583[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_584 (LSTM)                 (None, 1, 100)       80400       dropout_701[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_702 (Dropout)           (None, 1, 100)       0           lstm_584[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_585 (LSTM)                 (None, 1, 200)       240800      dropout_702[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_613 (Dense)               (None, 1)            2           input_377[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_703 (Dropout)           (None, 1, 200)       0           lstm_585[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_254 (Reshape)           (None, 1, 1)         0           dense_613[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_586 (LSTM)                 (None, 1, 100)       120400      dropout_703[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_588 (LSTM)                 (None, 1, 100)       40800       reshape_254[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_587 (LSTM)                 (None, 1, 1)         408         lstm_586[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_589 (LSTM)                 (None, 1, 1)         408         lstm_588[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_221 (Concatenate)   (None, 2, 1)         0           lstm_587[0][0]                   \n",
      "                                                                 lstm_589[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_614 (Dense)               (None, 2, 1)         2           concatenate_221[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 554,612\n",
      "Trainable params: 554,612\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "short_term_input1 = Input(shape=(5,))\n",
    "short_term_input2 = Input(shape=(5,))\n",
    "long_term_input1 = Input(shape=(1,))\n",
    "\n",
    "x1 = Dense(50)(short_term_input1)\n",
    "x1 = LeakyReLU(0.2)(x1)\n",
    "x1 = Dropout(0.35)(x1)\n",
    "\n",
    "x1 = Dense(50)(x1)\n",
    "x1 = LeakyReLU(0.2)(x1)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "\n",
    "x1 = Dense(50)(x1)\n",
    "x1 = LeakyReLU(0.2)(x1)\n",
    "\n",
    "\n",
    "con1 = Concatenate(axis=1)([x1, short_term_input2, long_term_input1])\n",
    "con1 = Dropout(0.35)(con1)\n",
    "\n",
    "con1 = Dense(56, activation = \"relu\")(con1)\n",
    "con1 = Dropout(0.2)(con1)\n",
    "\n",
    "con1 = Reshape(target_shape = (1, 56))(con1)\n",
    "\n",
    "con1 = LSTM(100, return_sequences = True)(con1)\n",
    "con1 = Dropout(0.2)(con1)\n",
    "\n",
    "con1 = LSTM(100, return_sequences = True)(con1)\n",
    "con1 = Dropout(0.35)(con1)\n",
    "\n",
    "con1 = LSTM(200, return_sequences = True)(con1)\n",
    "con1 = Dropout(0.35)(con1)\n",
    "con1 = LSTM(100, return_sequences = True)(con1)\n",
    "con1 = LSTM(1, return_sequences = True)(con1)\n",
    "\n",
    "x2 = Dense(1, activation = None)(long_term_input1)\n",
    "x2 = Reshape(target_shape= (1, 1))(x2)\n",
    "x2 = LSTM(100, return_sequences = True)(x2)\n",
    "x2 = LSTM(1, return_sequences = True)(x2)\n",
    "\n",
    "con2 = Concatenate(axis=1)([con1, x2]) \n",
    "\n",
    "output = Dense(1, activation = None)(con2)\n",
    "\n",
    "SingleLongTermmodel = Model(inputs=[short_term_input1, short_term_input2, long_term_input1], outputs=output)\n",
    "SingleLongTermmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0214 - root_mean_squared_error: 0.0288\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0205 - root_mean_squared_error: 0.0252\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0258 - root_mean_squared_error: 0.0319\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0212 - root_mean_squared_error: 0.0269\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0160 - root_mean_squared_error: 0.0227\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0211 - root_mean_squared_error: 0.0282\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0190 - root_mean_squared_error: 0.0254\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0231 - root_mean_squared_error: 0.0285\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0194 - root_mean_squared_error: 0.0256\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0193 - root_mean_squared_error: 0.0247\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0214 - root_mean_squared_error: 0.0305 \n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0232 - root_mean_squared_error: 0.0286\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0217 - root_mean_squared_error: 0.0289\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0186 - root_mean_squared_error: 0.0245\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0180 - root_mean_squared_error: 0.0243\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0263 - root_mean_squared_error: 0.0320\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0211 - root_mean_squared_error: 0.0265\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0219 - root_mean_squared_error: 0.0270\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0199 - root_mean_squared_error: 0.0262\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0179 - root_mean_squared_error: 0.0242\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0208 - root_mean_squared_error: 0.0269\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0203 - root_mean_squared_error: 0.0246\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0189 - root_mean_squared_error: 0.0256\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0238 - root_mean_squared_error: 0.0289\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0212 - root_mean_squared_error: 0.0285\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0178 - root_mean_squared_error: 0.0249\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0185 - root_mean_squared_error: 0.0252 \n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0187 - root_mean_squared_error: 0.0253\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0194 - root_mean_squared_error: 0.0247\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0206 - root_mean_squared_error: 0.0281\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.0188 - root_mean_squared_error: 0.0254\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0221 - root_mean_squared_error: 0.0278\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0209 - root_mean_squared_error: 0.0265\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0203 - root_mean_squared_error: 0.0283\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0181 - root_mean_squared_error: 0.0246\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0188 - root_mean_squared_error: 0.0248\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0182 - root_mean_squared_error: 0.0254\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0186 - root_mean_squared_error: 0.0250\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0185 - root_mean_squared_error: 0.0246\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0182 - root_mean_squared_error: 0.0245\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0185 - root_mean_squared_error: 0.0250\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0184 - root_mean_squared_error: 0.0248\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0188 - root_mean_squared_error: 0.0249 \n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0192 - root_mean_squared_error: 0.0264\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0190 - root_mean_squared_error: 0.0255\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0196 - root_mean_squared_error: 0.0262\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0188 - root_mean_squared_error: 0.0248 \n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0201 - root_mean_squared_error: 0.0258\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0186 - root_mean_squared_error: 0.0258\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0192 - root_mean_squared_error: 0.0246\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0196 - root_mean_squared_error: 0.0256\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0185 - root_mean_squared_error: 0.0261\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0214 - root_mean_squared_error: 0.0248\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0220 - root_mean_squared_error: 0.0308\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0261 - root_mean_squared_error: 0.0309\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0187 - root_mean_squared_error: 0.0245\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0186 - root_mean_squared_error: 0.0246\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0229 - root_mean_squared_error: 0.0296 \n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0184 - root_mean_squared_error: 0.0249\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0232 - root_mean_squared_error: 0.0276\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0205 - root_mean_squared_error: 0.0263\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0208 - root_mean_squared_error: 0.0260\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0185 - root_mean_squared_error: 0.0256\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0207 - root_mean_squared_error: 0.0260\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0190 - root_mean_squared_error: 0.0251\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0190 - root_mean_squared_error: 0.0253\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0185 - root_mean_squared_error: 0.0261\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0172 - root_mean_squared_error: 0.0239 \n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0212 - root_mean_squared_error: 0.0275\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0207 - root_mean_squared_error: 0.0255\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0206 - root_mean_squared_error: 0.0265\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0218 - root_mean_squared_error: 0.0273\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0220 - root_mean_squared_error: 0.0267\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0193 - root_mean_squared_error: 0.0238\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0191 - root_mean_squared_error: 0.0257\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0205 - root_mean_squared_error: 0.0265\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0184 - root_mean_squared_error: 0.0257\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0217 - root_mean_squared_error: 0.0279\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0194 - root_mean_squared_error: 0.0251\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0246 - root_mean_squared_error: 0.0301\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0211 - root_mean_squared_error: 0.0262\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0204 - root_mean_squared_error: 0.0260\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0185 - root_mean_squared_error: 0.0251\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0188 - root_mean_squared_error: 0.0254\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0196 - root_mean_squared_error: 0.0255\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0200 - root_mean_squared_error: 0.0268\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0210 - root_mean_squared_error: 0.0277\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0240 - root_mean_squared_error: 0.0304\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0183 - root_mean_squared_error: 0.0248\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0188 - root_mean_squared_error: 0.0261\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0190 - root_mean_squared_error: 0.0265\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0192 - root_mean_squared_error: 0.0262 \n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0188 - root_mean_squared_error: 0.0247\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0204 - root_mean_squared_error: 0.0269\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0183 - root_mean_squared_error: 0.0243\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0188 - root_mean_squared_error: 0.0241\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0190 - root_mean_squared_error: 0.0252\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0181 - root_mean_squared_error: 0.0248\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0192 - root_mean_squared_error: 0.0253\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0178 - root_mean_squared_error: 0.0243\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x2ad1725a7dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0321 - root_mean_squared_error: 0.0340\n"
     ]
    }
   ],
   "source": [
    "SingleLongTermmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='mae', metrics = [tf.keras.metrics.RootMeanSquaredError()])\n",
    "history = SingleLongTermmodel.fit([training_input_short_term_slice, training_input_short_term_slice, training_input_long_term1], training_output_slice, epochs = 100, batch_size = 1, verbose=4)\n",
    "history2 = SingleLongTermmodel.evaluate(x=[testing_input_short_term_slice, testing_input_short_term_slice, testing_input_long_term1], y=testing_output_slice, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean squared error"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Mean squared error (Metrics = Root mean Square error)\n",
    "\n",
    "SingleLongTermmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='mse', metrics = [tf.keras.metrics.RootMeanSquaredError()])\n",
    "history = SingleLongTermmodel.fit([training_input_short_term, training_input_short_term, training_input_long_term1], training_output, epochs = 1000, batch_size = 32, verbose = 1)\n",
    "history2 = SingleLongTermmodel.evaluate(x=[testing_input_short_term, testing_input_short_term, testing_input_long_term1], y=testing_output, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple long term variable"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "short_term_input1 = Input(shape=(5,))\n",
    "short_term_input2 = Input(shape=(5,))\n",
    "long_term_input1 = Input(shape=(1,))\n",
    "long_term_input2 = Input(shape=(1, ))\n",
    "x3 = Reshape(target_shape = (1, 1))(long_term_input2)\n",
    "\n",
    "x1 = Dense(50)(short_term_input1)\n",
    "x1 = LeakyReLU(0.2)(x1)\n",
    "x1 = Dropout(0.35)(x1)\n",
    "\n",
    "x1 = Dense(50)(x1)\n",
    "x1 = LeakyReLU(0.2)(x1)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "\n",
    "x1 = Dense(50)(x1)\n",
    "x1 = LeakyReLU(0.2)(x1)\n",
    "\n",
    "\n",
    "con1 = Concatenate(axis=1)([x1, short_term_input2, long_term_input1, long_term_input2])\n",
    "con1 = Dropout(0.35)(con1)\n",
    "\n",
    "con1 = Dense(56, activation = \"relu\")(con1)\n",
    "con1 = Dropout(0.2)(con1)\n",
    "\n",
    "con1 = Reshape(target_shape = (1, 56))(con1)\n",
    "\n",
    "con1 = LSTM(100, return_sequences = True)(con1)\n",
    "con1 = Dropout(0.2)(con1)\n",
    "\n",
    "con1 = LSTM(100, return_sequences = True)(con1)\n",
    "con1 = Dropout(0.35)(con1)\n",
    "\n",
    "con1 = LSTM(200, return_sequences = True)(con1)\n",
    "con1 = Dropout(0.35)(con1)\n",
    "con1 = LSTM(100, return_sequences = True)(con1)\n",
    "con1 = LSTM(1, return_sequences = True)(con1)\n",
    "\n",
    "\n",
    "con2 = Concatenate(axis=1)([long_term_input1, long_term_input2])\n",
    "x2 = Dense(16, activation = \"relu\")(con2)\n",
    "x2 = Reshape(target_shape= (1, 16))(x2)\n",
    "x2 = LSTM(100, return_sequences = True)(x2)\n",
    "x2 = LSTM(1, return_sequences = True)(x2)\n",
    "\n",
    "con2 = Concatenate(axis=1)([con1, x2, x3]) \n",
    "#con2 = Reshape(target_shape= (3, ))(con2)\n",
    "(con2) = Dense(100, activation = 'sigmoid')(con2)\n",
    "output = Dense(1, activation = None)(con2)\n",
    "\n",
    "MultipleInputmodel = Model(inputs=[short_term_input1, short_term_input2, long_term_input1, long_term_input2], outputs=output)\n",
    "MultipleInputmodel.summary()\n",
    "\n",
    "MultipleInputmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='mean_absolute_error', metrics = [tf.keras.metrics.RootMeanSquaredError()])\n",
    "#model.compile(optimizer= \"adam\", loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "history = MultipleInputmodel.fit([training_input_short_term, training_input_short_term, training_input_long_term1, training_input_long_term2], training_output, epochs = 10000, batch_size = 32, verbose = 1)\n",
    "history2 = model.evaluate(x=[testing_input_short_term, testing_input_short_term, testing_input_long_term1, testing_input_long_term2], y=testing_output, batch_size=32, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-env)",
   "language": "python",
   "name": "tensorflow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
